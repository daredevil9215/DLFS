{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $X$\n",
    "\n",
    "- kernel matrix $k$\n",
    "\n",
    "- output matrix $Y$\n",
    "\n",
    "$$Y = X \\star_{\\text{valid}} k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to input matrix $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to kernel $\\frac{\\partial L}{\\partial k}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\delta \\ast_{\\text{full}} k \\quad \\quad \\frac{\\partial L}{\\partial k} = X \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        output_height = int(floor((input_height - kernel_size + + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_width, output_height)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "            \n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        \"\"\"if self.padding:\n",
    "            input_shape = list(self.inputs.shape)\n",
    "            input_shape[2] += 2 * self.padding\n",
    "            input_shape[3] += 2 * self.padding\"\"\"\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=False)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=False)\n",
    "                            # Gradient with respect to inputs is the full convolution between delta and kernel\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "\n",
    "                            \n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, stride=False):\n",
    "\n",
    "        if stride:\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            delta_dilated_shape = delta_dilated.shape\n",
    "\n",
    "            input_shape = self.inputs.shape[-1]\n",
    "            kernel_shape = self.kernels_shape[-1]\n",
    "            padding = self.padding\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 2 * padding + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient is computed\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_shape - self.kernel_size + 2 * self.padding + 1, input_shape - kernel_shape + 2*self.padding + 1)\n",
    "                delta_dilated = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "        else:\n",
    "            dkernels = signal.correlate2d(inputs, delta, \"valid\")\n",
    "        return dkernels\n",
    "\n",
    "    def _calculate_input_gradient(self, delta: np.ndarray, kernel: np.ndarray, stride=False):\n",
    "        if stride:\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            dinputs = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "        else:\n",
    "            dinputs = signal.convolve2d(delta, kernel, \"full\")\n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape, output_shape) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # converts (batch_size, channels, width, height) to (batch_size, channels * width * height)\n",
    "        batch_size = inputs.shape[0]\n",
    "        #print(f'input: {inputs.shape}')\n",
    "        #print(f'output: {self.output_shape}')\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # converts (batch_size, channels * width * height) to (batch_size, channels, width, height)\n",
    "        #print(f'u backwardu reshape layera delta: {delta.shape}')\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))\n",
    "        #print(f'u backwardu reshape layera dinputs: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        self.output_height = int(floor((input_height - kernel_size + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # List for storing indices of max elements\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_width):\n",
    "                    for l in range(self.output_height):\n",
    "                        \n",
    "                        # Initalize axis 0 start and end indices \n",
    "                        axis_0_start = k * self.stride\n",
    "                        axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "\n",
    "                        if self.padding:\n",
    "                            arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                        else:\n",
    "                            arr = self.inputs[i, j].copy()\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "        #print(f'output maxpool: {self.output.shape}')\n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        #print(f'u backwardu maxpool layera delta: {delta.shape}')\n",
    "\n",
    "        # Initialize input gradient\n",
    "        \"\"\"if self.padding:\n",
    "            input_shape = list(self.inputs.shape)\n",
    "            input_shape[2] += 2 * self.padding\n",
    "            input_shape[3] += 2 * self.padding\n",
    "            input_shape = tuple(input_shape)\n",
    "        else:\"\"\"\n",
    "        input_shape = self.inputs.shape\n",
    "        \n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput = np.zeros((input_shape[2] + 2 * self.padding, input_shape[3] + 2 * self.padding))\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                \n",
    "\n",
    "        #print(f'u backwardu maxpool layera dinput: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.7513476563058694 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.3483895963083755 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.16693599515726126 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.1181061452064219 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.10380964611221756 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=1, padding=1), # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
    "          MaxPoolLayer(input_shape=(3, 28, 28), kernel_size=3, stride=2, padding=2), # (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "          ConvolutionalLayer(input_shape=(3, 15, 15), output_channels=4, kernel_size=3, stride=3, padding=0), # (15 - 3 + 2 * 0) / 3 + 1 = 5\n",
    "          MaxPoolLayer(input_shape=(4, 5, 5), kernel_size=3, stride=2, padding=1),  # (5 - 3 + 2 * 1) / 2 + 1 = 3\n",
    "          ReshapeLayer(input_shape=(4, 3, 3), output_shape=4*3*3),\n",
    "          DenseLayer(4*3*3, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2563.7981664676627 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2357.9773458868967 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2278.1847484648106 =====\n",
      "===== EPOCH : 15 ===== LOSS : 2259.3798997479726 =====\n",
      "===== EPOCH : 20 ===== LOSS : 2245.083101635274 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=1, padding=1), # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
    "          MaxPoolLayer(input_shape=(3, 28, 28), kernel_size=3, stride=2, padding=2), # (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "          ConvolutionalLayer(input_shape=(3, 15, 15), output_channels=4, kernel_size=3, stride=3, padding=3), # (15 - 3 + 2 * 3) / 3 + 1 = 7\n",
    "          MaxPoolLayer(input_shape=(4, 7, 7), kernel_size=3, stride=2, padding=1),  # (7 - 3 + 2 * 1) / 2 + 1 = 4\n",
    "          ReshapeLayer(input_shape=(4, 4, 4), output_shape=4*4*4),\n",
    "          DenseLayer(4*4*4, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
