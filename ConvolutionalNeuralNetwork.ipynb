{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $X$\n",
    "\n",
    "- kernel matrix $k$\n",
    "\n",
    "- output matrix $Y$\n",
    "\n",
    "$$Y = X \\star_{\\text{valid}} k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to input matrix $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to kernel $\\frac{\\partial L}{\\partial k}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\delta \\ast_{\\text{full}} k \\quad \\quad \\frac{\\partial L}{\\partial k} = X \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        output_height = int(floor((input_height - kernel_size + + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_width, output_height)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "            \n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        \"\"\"if self.padding:\n",
    "            input_shape = list(self.inputs.shape)\n",
    "            input_shape[2] += 2 * self.padding\n",
    "            input_shape[3] += 2 * self.padding\"\"\"\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=False)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=False)\n",
    "                            # Gradient with respect to inputs is the full convolution between delta and kernel\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "\n",
    "                            \n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, stride=False):\n",
    "\n",
    "        if stride:\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            delta_dilated_shape = delta_dilated.shape\n",
    "\n",
    "            input_shape = self.inputs.shape[-1]\n",
    "            kernel_shape = self.kernels_shape[-1]\n",
    "            padding = self.padding\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 2 * padding + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient is computed\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_shape - self.kernel_size + 2 * self.padding + 1, input_shape - kernel_shape + 2*self.padding + 1)\n",
    "                delta_dilated = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "        else:\n",
    "            dkernels = signal.correlate2d(inputs, delta, \"valid\")\n",
    "        return dkernels\n",
    "\n",
    "    def _calculate_input_gradient(self, delta: np.ndarray, kernel: np.ndarray, stride=False):\n",
    "        if stride:\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            dinputs = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "        else:\n",
    "            dinputs = signal.convolve2d(delta, kernel, \"full\")\n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape, output_shape) -> None:\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # converts (batch_size, channels, width, height) to (batch_size, channels * width * height)\n",
    "        batch_size = inputs.shape[0]\n",
    "        #print(f'input: {inputs.shape}')\n",
    "        #print(f'output: {self.output_shape}')\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta):\n",
    "        # converts (batch_size, channels * width * height) to (batch_size, channels, width, height)\n",
    "        #print(f'u backwardu reshape layera delta: {delta.shape}')\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))\n",
    "        #print(f'u backwardu reshape layera dinputs: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        self.output_height = int(floor((input_height - kernel_size + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # List for storing indices of max elements\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_width):\n",
    "                    for l in range(self.output_height):\n",
    "                        \n",
    "                        # Initalize axis 0 start and end indices \n",
    "                        axis_0_start = k * self.stride\n",
    "                        axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "\n",
    "                        if self.padding:\n",
    "                            arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                        else:\n",
    "                            arr = self.inputs[i, j].copy()\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "        #print(f'output maxpool: {self.output.shape}')\n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        #print(f'u backwardu maxpool layera delta: {delta.shape}')\n",
    "\n",
    "        # Initialize input gradient\n",
    "        \"\"\"if self.padding:\n",
    "            input_shape = list(self.inputs.shape)\n",
    "            input_shape[2] += 2 * self.padding\n",
    "            input_shape[3] += 2 * self.padding\n",
    "            input_shape = tuple(input_shape)\n",
    "        else:\"\"\"\n",
    "        input_shape = self.inputs.shape\n",
    "        \n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput = np.zeros((input_shape[2] + 2 * self.padding, input_shape[3] + 2 * self.padding))\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                \n",
    "\n",
    "        #print(f'u backwardu maxpool layera dinput: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.7594450905767337 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.7037293486809051 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.5541463109859015 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.376960051482755 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.3042224472492678 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=1, padding=1), # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
    "          MaxPoolLayer(input_shape=(3, 28, 28), kernel_size=3, stride=2, padding=2), # (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "          ConvolutionalLayer(input_shape=(3, 15, 15), output_channels=4, kernel_size=3, stride=3, padding=0), # (15 - 3 + 2 * 0) / 3 + 1 = 5\n",
    "          MaxPoolLayer(input_shape=(4, 5, 5), kernel_size=3, stride=2, padding=1),  # (5 - 3 + 2 * 1) / 2 + 1 = 3\n",
    "          ReshapeLayer(input_shape=(4, 3, 3), output_shape=4*3*3),\n",
    "          DenseLayer(4*3*3, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=0))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2447.6514702739673 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2325.039786804998 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2263.772286928899 =====\n",
      "===== EPOCH : 15 ===== LOSS : 2246.3434971685883 =====\n",
      "===== EPOCH : 20 ===== LOSS : 2216.3251352119014 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=1, padding=1), # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
    "          MaxPoolLayer(input_shape=(3, 28, 28), kernel_size=3, stride=2, padding=2), # (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "          ConvolutionalLayer(input_shape=(3, 15, 15), output_channels=4, kernel_size=3, stride=3, padding=3), # (15 - 3 + 2 * 3) / 3 + 1 = 7\n",
    "          MaxPoolLayer(input_shape=(4, 7, 7), kernel_size=3, stride=2, padding=1),  # (7 - 3 + 2 * 1) / 2 + 1 = 4\n",
    "          ReshapeLayer(input_shape=(4, 4, 4), output_shape=4*4*4),\n",
    "          DenseLayer(4*4*4, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
