{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $X$\n",
    "\n",
    "- kernel matrix $k$\n",
    "\n",
    "- output matrix $Y$\n",
    "\n",
    "$$Y = X \\star_{\\text{valid}} k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to input matrix $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to kernel $\\frac{\\partial L}{\\partial k}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\delta \\ast_{\\text{full}} k \\quad \\quad \\frac{\\partial L}{\\partial k} = X \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "        output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_width, output_height)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "            \n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            # Loop through output and input channels\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=False)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                            # If padding is present dinput should be unpadded\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "                            # If padding is present dinput should be unpadded\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=False)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "\n",
    "                    # Update gradients for the current sample, output and input channel\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, stride: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating kernel gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Current accumulated gradient.\n",
    "\n",
    "        stride : bool, default=False\n",
    "            Checks whether stride is present.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dkernel : np.ndarray\n",
    "            Kernel gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride:\n",
    "\n",
    "            # If stride is present delta must be dilated\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            delta_dilated_shape = delta_dilated.shape\n",
    "\n",
    "            # Get input and kernel shape (assumes they are square matrices)\n",
    "            input_shape = self.inputs.shape[-1]\n",
    "            kernel_shape = self.kernels_shape[-1]\n",
    "            padding = self.padding\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 2 * padding + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient is computed\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_shape - self.kernel_size + 2 * self.padding + 1, input_shape - kernel_shape + 2*self.padding + 1)\n",
    "                delta_dilated = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "        else:\n",
    "            # Gradient with respect to kernel is valid cross correlation between input and delta\n",
    "            dkernels = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernels\n",
    "\n",
    "    def _calculate_input_gradient(self, delta: np.ndarray, kernel: np.ndarray, stride: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating input gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Current accumulated gradient.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Current kernel.\n",
    "\n",
    "        stride : bool, default=False\n",
    "            Checks whether stride is present.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dinputs : np.ndarray\n",
    "           Input gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride:\n",
    "            # If stride is present delta must be dilated\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            dinputs = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "        else:\n",
    "            # Gradient with respect to input is a full convolution between delta and kernel\n",
    "            dinputs = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int, int], output_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple[int, int, int]\n",
    "            Input shape of a single sample. For images it's (channels, width, height).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels, width, height) to (batch_size, channels * width * height). Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels * width * height) to (batch_size, channels, width, height). Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        self.output_height = int(floor((input_height - kernel_size + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # List for storing indices of max elements\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_width):\n",
    "                    for l in range(self.output_height):\n",
    "                        \n",
    "                        # Initalize axis 0 start and end indices \n",
    "                        axis_0_start = k * self.stride\n",
    "                        axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "\n",
    "                        if self.padding:\n",
    "                            arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                        else:\n",
    "                            arr = self.inputs[i, j].copy()\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "        #print(f'output maxpool: {self.output.shape}')\n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        # Initialize input gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                dinput = np.zeros((input_shape[2] + 2 * self.padding, input_shape[3] + 2 * self.padding))\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()\n",
    "                \n",
    "\n",
    "        #print(f'u backwardu maxpool layera dinput: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.7020156705748242 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.17241739906643358 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.026122536598734092 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.004876565234313089 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.0013911958127651642 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3), # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
    "          MaxPoolLayer(input_shape=(3, 26, 26), kernel_size=3, stride=2), # (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "          ConvolutionalLayer(input_shape=(3, 12, 12), output_channels=4, kernel_size=3), # (15 - 3 + 2 * 0) / 3 + 1 = 5\n",
    "          MaxPoolLayer(input_shape=(4, 10, 10), kernel_size=3, stride=2),  # (5 - 3 + 2 * 1) / 2 + 1 = 3\n",
    "          ReshapeLayer(input_shape=(4, 4, 4), output_shape=4*4*4),\n",
    "          DenseLayer(4*4*4, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGFCAYAAABg2vAPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlT0lEQVR4nO3deXBV9f3/8dcFYsKShaRBDEtoBAQLFEe2MtGAIItEhGEZW3c70CpqakFERMD5CjSFgBV0ZJRFxNpCBGrFtQKKigTUoFCWEEESgiCiWUBhMOf3hz9S4+eE3HCX5OTzfMzwh28+55z3Zd6El+d+7rk+x3EcAQAAazWo7QYAAEDtIgwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJazNgwcPHhQPp9P8+bNC9o5N23aJJ/Pp02bNgXtnMDPMbvwKma37vJUGFi+fLl8Pp+2b99e262ExN69e3X//ferb9++ioqKks/n08GDB2t0jt27d2vIkCFq1qyZ4uPjdcstt+irr74KTcPwG7NbPWa3bqrvsytJhw8f1tixYxUXF6eYmBjdcMMN+vzzz/0+/oMPPlBqaqqaNGmili1b6r777lNZWVkIOw6+RrXdAP5ny5YteuKJJ3T55Zerc+fOys3NrdHxhYWFuvrqqxUbG6vZs2errKxM8+bN02effaacnBxddNFFoWkc1mN24VVlZWXq37+/iouLNXXqVEVERGjBggVKS0tTbm6uEhISznt8bm6uBgwYoM6dO2v+/PkqLCzUvHnzlJeXp9deey1MryJwhIE6ZPjw4fr2228VHR2tefPm1fgH6uzZs3Xy5El99NFHatu2rSSpV69euvbaa7V8+XKNHz8+BF0DzC6866mnnlJeXp5ycnLUs2dPSdLQoUPVpUsXZWVlafbs2ec9furUqWrevLk2bdqkmJgYSVK7du00btw4vfnmmxo0aFDIX0MweOptAn+cOXNG06dP15VXXqnY2Fg1bdpUV111lTZu3FjlMQsWLFBycrIaN26stLQ07dy501izZ88ejR49WvHx8YqKilKPHj308ssvV9vPqVOntGfPHh0/frzatfHx8YqOjq52XVVeeuklpaenV/wwlaSBAweqY8eOWrVq1QWfF+HB7DK7XuXl2c3OzlbPnj0rgoAkderUSQMGDKh29kpKSvTWW2/p5ptvrggCknTrrbeqWbNmnprdehcGSkpK9Oyzz6pfv37KzMzUzJkz9dVXX2nw4MGu/7eyYsUKPfHEE5owYYIeeugh7dy5U9dcc42OHj1asWbXrl3q06ePdu/erSlTpigrK0tNmzbViBEjtHbt2vP2k5OTo86dO2vRokXBfqmVHD58WMeOHVOPHj2M3+vVq5c++eSTkF4fgWN2mV2v8urslpeX69NPP61y9vLz81VaWlrl8Z999pnOnj1rHH/RRRepe/funprdevc2QfPmzXXw4MFK7zGOGzdOnTp10sKFC7VkyZJK6/fv36+8vDy1atVKkjRkyBD17t1bmZmZmj9/viQpIyNDbdu21bZt2xQZGSlJuvvuu5WamqoHH3xQI0eODNOrq9qRI0ckSZdcconxe5dccolOnDih06dPV/SPuofZZXa9yquze262qpo9SSoqKtJll13menx1s7t58+aAewyXendnoGHDhhUDWV5erhMnTlQkt48//thYP2LEiIqBlH5Mg71799arr74q6cdh2bBhg8aOHavS0lIdP35cx48f19dff63BgwcrLy9Phw8frrKffv36yXEczZw5M7gv9Ge+++47SXL9gRkVFVVpDeomZpfZ9Sqvzm6gs1fd8V6a23oXBiTpueeeU7du3RQVFaWEhAQlJiZq/fr1Ki4uNtZ26NDBqHXs2LHiY1H79++X4zh65JFHlJiYWOnXjBkzJEnHjh0L6evxR+PGjSVJp0+fNn7v+++/r7QGdRezWxmz6x1enN1AZ6+64700t/XubYKVK1fq9ttv14gRI/TAAw+oRYsWatiwoebMmaP8/Pwan6+8vFySNGnSJA0ePNh1Tfv27QPqORjO3aY6d9vqp44cOaL4+Hhus9ZxzC6z61Vend1zs1XV7ElSUlJSlcdXN7vnO7auqXdhIDs7WykpKVqzZo18Pl9F/Vya/Lm8vDyjtm/fPrVr106SlJKSIkmKiIjQwIEDg99wkLRq1UqJiYmuDwbJyclR9+7dw98UaoTZZXa9yquz26BBA3Xt2tV19rZu3aqUlJTzfkqmS5cuatSokbZv366xY8dW1M+cOaPc3NxKtbqu3r1N0LBhQ0mS4zgVta1bt2rLli2u69etW1fpvaecnBxt3bpVQ4cOlSS1aNFC/fr10+LFi13TX3VPSKvJR1xqIj8/30jco0aN0iuvvKKCgoKK2ttvv619+/ZpzJgxQb0+go/ZZXa9ysuzO3r0aG3btq1SINi7d682bNhgzN6ePXt06NChiv+OjY3VwIEDtXLlykqfOnj++edVVlbmqdn15J2BpUuX6vXXXzfqGRkZSk9P15o1azRy5EgNGzZMBw4c0NNPP63LL7/c9fGQ7du3V2pqqu666y6dPn1ajz/+uBISEjR58uSKNU8++aRSU1PVtWtXjRs3TikpKTp69Ki2bNmiwsJC7dixo8pec3Jy1L9/f82YMaPazSzFxcVauHChJOn999+XJC1atEhxcXGKi4vTPffcU7F2wIABklTpka9Tp07V6tWr1b9/f2VkZKisrExz585V165ddccdd5z32ggPZpfZ9ar6Ort33323nnnmGQ0bNkyTJk1SRESE5s+fr4svvlgTJ06stLZz585KS0ur9D0Is2bNUt++fZWWlqbx48ersLBQWVlZGjRokIYMGXLea9cpjocsW7bMkVTlr4KCAqe8vNyZPXu2k5yc7ERGRjpXXHGF88orrzi33Xabk5ycXHGuAwcOOJKcuXPnOllZWU6bNm2cyMhI56qrrnJ27NhhXDs/P9+59dZbnZYtWzoRERFOq1atnPT0dCc7O7tizcaNGx1JzsaNG43ajBkzqn1953py+/XT3h3HcZKTk42a4zjOzp07nUGDBjlNmjRx4uLinJtuusn58ssvq702QovZ/R9m11vq++w6juMUFBQ4o0ePdmJiYpxmzZo56enpTl5enrFOkpOWlmbUN2/e7PTt29eJiopyEhMTnQkTJjglJSV+Xbuu8DnOT+7rAAAA69S7PQMAAKBmCAMAAFiOMAAAgOUIAwAAWI4wAACA5QgDAABYzq+HDpWXl6uoqEjR0dGVHjUJ1ITjOCotLVVSUpIaNAhPDmV2EQzMLrzK39n1KwwUFRWpTZs2QWsOdisoKFDr1q3Dci1mF8HE7MKrqptdvyLu+b6oAaipcM4Ts4tgYnbhVdXNk19hgFtUCKZwzhOzi2BiduFV1c0TGwgBALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwHGEAAADLEQYAALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwHGEAAADLEQYAALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwXKPabsBGo0aNcq1PnjzZqPXu3TvU7aAe2Ldvn2v90ksvNWoxMTFG7eTJk0HvCQi2zMxMo+Y2u88884xRO3LkSEh6qi+4MwAAgOUIAwAAWI4wAACA5QgDAABYjg2EIda9e3ej5ra5RZJ8Pl+Iu0F95TiO3/WRI0catZUrVwa9JyDYRowYYdQ6dOhg1DIyMozawoULXc/ptinxu+++q3lzHsedAQAALEcYAADAcoQBAAAsRxgAAMBybCAMsd/85jdGLS4uznXtunXrQtsM6q1vvvnG77XLly83amwgRF3jtvm6devWfh3bvHlzozZ9+nTXtV988YVRW7ZsmV/XqU+4MwAAgOUIAwAAWI4wAACA5QgDAABYjg2EIRYVFeX32rVr14awE9Rnc+bMca0/8sgjRq1bt25GrWXLlkbtyy+/DLwxoBpJSUmu9X/84x9GrXHjxkG//uLFi41aWVmZUVu9enXQr12XcGcAAADLEQYAALAcYQAAAMsRBgAAsBxhAAAAy/FpghC7+eab/V67bdu2EHaC+uxf//qX32tXrVpl1Nw+dTBhwoSAegL8cf3117vWO3bsGJbrN2pk/jPo9umcV1991aidPHkyJD3VBu4MAABgOcIAAACWIwwAAGA5wgAAAJZjA2EQuT3mtWvXrkYtJyfH9fj9+/cHvSfYrW3btkatYcOGRs1toysbCBFscXFxRi0zMzOgc+7atcuouc3zHXfc4Xr8XXfdZdRSUlKM2hVXXGHU3nvvPX9a9ATuDAAAYDnCAAAAliMMAABgOcIAAACWYwNhEA0fPtyouT3d6syZM67Hnz17Nug9wW533nlnbbcAVLj00kuNWkxMTEDn/Otf/2rUduzYYdT+9Kc/uR5fWlpq1B5++GGjNmrUKKPGBkIAAFBvEAYAALAcYQAAAMsRBgAAsBwbCIMoKSnJr3UrVqwIcSfAj9auXWvU3J6KCYSD2ya8migsLDRqf//73wM651/+8hej9sc//tGo3XvvvUbt5ZdfNmobN24MqJ/awp0BAAAsRxgAAMByhAEAACxHGAAAwHKEAQAALMenCYLo+uuvN2o//PCDUXPbgQqEgtv3sgPhcPHFFxu122+/PaBzrly50qi5/YytiZMnTxq1DRs2GLUxY8YYtWHDhhk1Pk0AAAA8iTAAAIDlCAMAAFiOMAAAgOXYQHiB+vTpY9TcHkf8zjvvGLVjx46FpCfg56699lq/1jVr1syo/eEPfzBqixcvDrgn2OGSSy4xai1btvT7+F27dhm1xx57LKCeUDXuDAAAYDnCAAAAliMMAABgOcIAAACWYwPhBZoyZYpR8/l8Rm337t3haAdwdcMNNxg1t01YgwcPNmoDBw40amwghL9SU1MDOv61114zaqdOnQronG4uvfRSo+b2tEE3bj/zvYo7AwAAWI4wAACA5QgDAABYjjAAAIDl2EB4gRISEmq7BaBa27dvN2puT8V0e1Kh21M2AX/98pe/DOj4zMzMIHVyfpMnT77gYx3HCWIntYs7AwAAWI4wAACA5QgDAABYjjAAAIDl2EAYRG5fTfzUU0/VQicAULs6duzo17q3337btf7NN98Esx1lZWW51m+88cYLPqfbUxK9ijsDAABYjjAAAIDlCAMAAFiOMAAAgOUIAwAAWI5PE/ihVatWRq1Tp05G7YsvvjBqu3btCklPAFCXnT592q91J06ccK2Xl5df8LXnzJlj1MaPH++6tmnTpn6d8+DBg0bto48+qlFfdRl3BgAAsBxhAAAAyxEGAACwHGEAAADLsYHQDy1btjRqv/jFL4xauL5/GwDqOrcN1W5iY2Nd6+3btzdqV155pVGbNm2aUfvVr37l17Vr4j//+Y9R+/bbb4N+ndrCnQEAACxHGAAAwHKEAQAALEcYAADAcmwg9MNvf/tbo+Y4jlE7efJkONoBAnLzzTf7tS4qKsqotW7d2nVtYWFhQD2h/tm9e7df6wYNGuRa/+STT4xao0bmP1mRkZE1a8wPn3/+uVF74YUXgn6duoQ7AwAAWI4wAACA5QgDAABYjjAAAIDl2EDoh1tuucWo+Xw+o7Zt27ZwtAMEpKyszK91zZs3N2oDBw50Xbt8+fJAWkI95O8T++Li4lyP9/erhd24/Xx22/RdlUWLFhm1d95554L78QLuDAAAYDnCAAAAliMMAABgOcIAAACWYwOhH9yecOX2hCq3J2YBdc26deuMWo8ePcLfCOq1gwcPGrU///nPRm3p0qVBv3ZNNgvOmjXLqC1cuDCY7XgCdwYAALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwHJ8muEDvvvuuUfvhhx9qoRMA8IZVq1YZtYyMDNe1v/71ry/4Ot98841Rq+oTApmZmUbNxp/l3BkAAMByhAEAACxHGAAAwHKEAQAALMcGwgs0bNgwo9aokfnHefbs2XC0A/hty5YtRu2DDz4wat26dTNqn376aUh6gh1OnTpl1AYPHuy69ve//71RGzp0qFHLz883alOnTjVqR44c8adFa3FnAAAAyxEGAACwHGEAAADLEQYAALCcz/Hji59LSkoUGxsbjn5ggeLiYsXExITlWswugonZhVdVN7vcGQAAwHKEAQAALEcYAADAcoQBAAAsRxgAAMByhAEAACxHGAAAwHKEAQAALEcYAADAcoQBAAAsRxgAAMByhAEAACxHGAAAwHKEAQAALEcYAADAcoQBAAAsRxgAAMByhAEAACxHGAAAwHJ+hQHHcULdBywSznlidhFMzC68qrp58isMlJaWBqUZQArvPDG7CCZmF15V3Tz5HD/iZ3l5uYqKihQdHS2fzxe05mAXx3FUWlqqpKQkNWgQnneomF0EA7MLr/J3dv0KAwAAoP5iAyEAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDlrw8DBgwfl8/k0b968oJ1z06ZN8vl82rRpU9DOCfwcswuvYnbrLk+FgeXLl8vn82n79u213UrIHD58WGPHjlVcXJxiYmJ0ww036PPPP/f7+A8++ECpqalq0qSJWrZsqfvuu09lZWUh7Bj+qO+zu3fvXt1///3q27evoqKi5PP5dPDgwRqdY/fu3RoyZIiaNWum+Ph43XLLLfrqq69C0zD8xuxWrz7MbqPabgD/U1ZWpv79+6u4uFhTp05VRESEFixYoLS0NOXm5iohIeG8x+fm5mrAgAHq3Lmz5s+fr8LCQs2bN095eXl67bXXwvQqYKMtW7boiSee0OWXX67OnTsrNze3RscXFhbq6quvVmxsrGbPnq2ysjLNmzdPn332mXJycnTRRReFpnFYj9n9EWGgDnnqqaeUl5ennJwc9ezZU5I0dOhQdenSRVlZWZo9e/Z5j586daqaN2+uTZs2KSYmRpLUrl07jRs3Tm+++aYGDRoU8tcAOw0fPlzffvutoqOjNW/evBr/QJ09e7ZOnjypjz76SG3btpUk9erVS9dee62WL1+u8ePHh6BrgNk9x1NvE/jjzJkzmj59uq688krFxsaqadOmuuqqq7Rx48Yqj1mwYIGSk5PVuHFjpaWlaefOncaaPXv2aPTo0YqPj1dUVJR69Oihl19+udp+Tp06pT179uj48ePVrs3OzlbPnj0rgoAkderUSQMGDNCqVavOe2xJSYneeust3XzzzRVBQJJuvfVWNWvWrNrjUfu8PLvx8fGKjo6udl1VXnrpJaWnp1f8MJWkgQMHqmPHjsyuBzC73p/dehcGSkpK9Oyzz6pfv37KzMzUzJkz9dVXX2nw4MGuiW/FihV64oknNGHCBD300EPauXOnrrnmGh09erRiza5du9SnTx/t3r1bU6ZMUVZWlpo2baoRI0Zo7dq15+0nJydHnTt31qJFi867rry8XJ9++ql69Ohh/F6vXr2Un5+v0tLSKo//7LPPdPbsWeP4iy66SN27d9cnn3xy3uuj9nl1dgN1+PBhHTt2rMrZZ3brPmbX+7Nb794maN68uQ4ePFjpfZpx48apU6dOWrhwoZYsWVJp/f79+5WXl6dWrVpJkoYMGaLevXsrMzNT8+fPlyRlZGSobdu22rZtmyIjIyVJd999t1JTU/Xggw9q5MiRAfd94sQJnT59Wpdcconxe+dqRUVFuuyyy1yPP3LkSKW1Pz9+8+bNAfeI0PLq7Aaqutk993fjXP+oe5hd789uvbsz0LBhw4qBLC8v14kTJyr+j/njjz821o8YMaJiIKUf01zv3r316quvSvrxH+kNGzZo7NixKi0t1fHjx3X8+HF9/fXXGjx4sPLy8nT48OEq++nXr58cx9HMmTPP2/d3330nSa5DExUVVWnNhRx/vmNRN3h1dgMV6Oyj9jG73p/dehcGJOm5555Tt27dFBUVpYSEBCUmJmr9+vUqLi421nbo0MGodezYseKjJfv375fjOHrkkUeUmJhY6deMGTMkSceOHQu458aNG0uSTp8+bfze999/X2nNhRx/vmNRd3hxdgMV6OyjbmB2K/Pa7Na7twlWrlyp22+/XSNGjNADDzygFi1aqGHDhpozZ47y8/NrfL7y8nJJ0qRJkzR48GDXNe3btw+oZ+nHTSyRkZEVt51+6lwtKSmpyuPP3aaq6vjzHYu6wauzG6jqZvfc3w3UXcyu92e33oWB7OxspaSkaM2aNfL5fBX1c2ny5/Ly8ozavn371K5dO0lSSkqKJCkiIkIDBw4MfsP/X4MGDdS1a1fXB3ts3bpVKSkp593x2qVLFzVq1Ejbt2/X2LFjK+pnzpxRbm5upRrqJq/ObqBatWqlxMRE19nPyclR9+7dw98UaoTZ9f7s1ru3CRo2bChJchynorZ161Zt2bLFdf26desqvfeUk5OjrVu3aujQoZKkFi1aqF+/flq8eLFr+qvuKVM1+YjL6NGjtW3btkqDtXfvXm3YsEFjxoyptHbPnj06dOhQxX/HxsZq4MCBWrlyZaVPHTz//PMqKyszjkfd4+XZrYn8/Hzj/xZHjRqlV155RQUFBRW1t99+W/v27WN2PYDZ9f7sevLOwNKlS/X6668b9YyMDKWnp2vNmjUaOXKkhg0bpgMHDujpp5/W5Zdf7vpY3vbt2ys1NVV33XWXTp8+rccff1wJCQmaPHlyxZonn3xSqamp6tq1q8aNG6eUlBQdPXpUW7ZsUWFhoXbs2FFlrzk5Oerfv79mzJhR7WaWu+++W88884yGDRumSZMmKSIiQvPnz9fFF1+siRMnVlrbuXNnpaWlVXoe96xZs9S3b1+lpaVp/PjxKiwsVFZWlgYNGqQhQ4ac99oIj/o6u8XFxVq4cKEk6f3335ckLVq0SHFxcYqLi9M999xTsXbAgAGSVOmRr1OnTtXq1avVv39/ZWRkqKysTHPnzlXXrl11xx13nPfaCA9mt57PruMhy5YtcyRV+augoMApLy93Zs+e7SQnJzuRkZHOFVdc4bzyyivObbfd5iQnJ1ec68CBA44kZ+7cuU5WVpbTpk0bJzIy0rnqqqucHTt2GNfOz893br31Vqdly5ZORESE06pVKyc9Pd3Jzs6uWLNx40ZHkrNx40ajNmPGDL9eY0FBgTN69GgnJibGadasmZOenu7k5eUZ6yQ5aWlpRn3z5s1O3759naioKCcxMdGZMGGCU1JS4te1ETr1fXbP9eT266e9O47jJCcnGzXHcZydO3c6gwYNcpo0aeLExcU5N910k/Pll19We22EFrP7P/V5dn2O85P7OgAAwDr1bs8AAACoGcIAAACWIwwAAGA5wgAAAJYjDAAAYDm/njNQXl6uoqIiRUdHV3q6FFATjuOotLRUSUlJatAgPDmU2UUwMLvwKn9n168wUFRUpDZt2gStOditoKBArVu3Dsu1mF0EE7MLr6pudv2KuOd7Jj5QU+GcJ2YXwcTswquqmye/wgC3qBBM4ZwnZhfBxOzCq6qbJzYQAgBgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5wgAAAJYjDAAAYDnCAAAAlmtU2w3UlnXr1hm166+/PujXadDAzFuPPvqo69ri4mKjtn79eqO2b9++wBuDtSIjI43a66+/btTc5nTTpk2haAkIqi5duhi1N954w3VtkyZNjFrz5s2D3lNdx50BAAAsRxgAAMByhAEAACxHGAAAwHL1bgOh22aQxYsXGzW3zYKO4wS9n/LycqM2bdo0v4+/6667jNrDDz9s1FavXl2zxmCtM2fOGLXt27cbNbcNVwkJCa7nLCsrC7wxIEhuv/12o9ayZUvXtW4bt23EnQEAACxHGAAAwHKEAQAALEcYAADAcj7Hj11zJSUlio2NDUc/AUtNTTVqbk9N8/l8Ri0UGwjDdR231/3xxx+7rnXbQBZOxcXFiomJCcu1vDS74RIdHW3UCgsL/VpX1Z9laWlp4I15ALPrDW4bt6v6ueu2gTA+Pj7oPdW26maXOwMAAFiOMAAAgOUIAwAAWI4wAACA5Tz7BEK3r2GVpMmTJ4e5k7rh/fffN2pVPelwzpw5oW4HdZjbptZmzZrVQidA4IYMGRLQ8VV9tbFtuDMAAIDlCAMAAFiOMAAAgOUIAwAAWI4wAACA5Tz7aYIrrrjCtX7dddeFuZO6q1u3brXdAgAEjdvjdCdMmODXsSdOnHCtP/300wH1VF9wZwAAAMsRBgAAsBxhAAAAyxEGAACwnGc3EFbF7VGrbho0MHOQ23dgS1JRUZFRS09PN2o7duwwamlpaUZt4sSJrtcZNmyYa90fbq9n7Nixrmtzc3ONWmZm5gVfG/WTv3+XgHBx2zju76bx9957z7X+zjvvBNRTfcGdAQAALEcYAADAcoQBAAAsRxgAAMBynthA2KZNG6P2+OOPu651HMevc7ptFjx06JDr2jFjxhg1t82Cbtw2p5w+fdp1be/evY1aQkKCX9dxez1V/VncdtttRu2FF14waoWFhX5dG97iNn9uc9q/f/9wtAP47f/+7/8u+NhHHnkkiJ3UP9wZAADAcoQBAAAsRxgAAMByhAEAACzniQ2Effr0MWo9evQI+nU+/PBD1/r27dvDcp0777zTqC1dutSo+bupsCodO3Y0am5/xtnZ2QFdB3VTYmKiUXN7UqbbBtTRo0e7nnPZsmWBNwYECU/PrDnuDAAAYDnCAAAAliMMAABgOcIAAACWIwwAAGA5T3yaIFxWrFhRq9dfv369Udu0aZNRGzVqVNCvHRsbG/RzovbFxcUZNbc58xePqIYXfP3110bt+++/r4VOvIM7AwAAWI4wAACA5QgDAABYjjAAAIDlPLGBcPjw4WG5zqeffhqW69RF06ZNM2pLliyphU4QTE2bNjVqXbp0ueDzVfUobaAu+ec//2nU9u/fXwudeAd3BgAAsBxhAAAAyxEGAACwHGEAAADLeWIDodt3rQf6fdUNGng3B7m9drfXU15e7vc5vfzngZrx9+/ONddcY9RKS0uD3Q7gN7fZdau999574WinXuFfAAAALEcYAADAcoQBAAAsRxgAAMBynthAmJuba9SSkpICOmdNNtfVJrfX7vYVxm6vx3Ecv6+zbt26mrQFD/N3LgYNGmTU3L5SGwiFjIwMo9a9e3ej5jbPL774Yihaqte4MwAAgOUIAwAAWI4wAACA5QgDAABYzhMbCJ9//nmjdt111wX9Ovfee69rfcqUKUG/lr82b94cluu4fU202wYe2OPNN9+s7RZggaioKNf6o48+atSaNGkS6nasxZ0BAAAsRxgAAMByhAEAACxHGAAAwHKEAQAALOeJTxOEy4033uha//e//23UPvnkE6N26tQpv65T1Y7YmTNnGrWxY8f6dc5AFRcXh+U6CK+SkhKj9uGHHxq1Pn36GDUeR4xwmD59ums9Ojo6zJ3YjTsDAABYjjAAAIDlCAMAAFiOMAAAgOU8sYFwx44dRu2LL75wXZucnHzB12ndurVr/Z133jFq69evN2p5eXlGzefzGbUOHTq4XicUj1j212OPPVZr10boxMTEGDW3zYJAfTJr1iyj9vDDD9dCJ97BnQEAACxHGAAAwHKEAQAALEcYAADAcp7YQLhv3z6jdvToUde17dq18+ucDRqYOai8vNzvntLT08NyHX/V5DoffPCBUcvOzg56T6h9x48fN2ovv/yyURs+fLhRW7JkSUh6AkLtxRdfrO0WPIc7AwAAWI4wAACA5QgDAABYjjAAAIDlPLGB0E1ubq5rvVevXn4d77a5znGcQFqqc9c5dOiQ69r7778/6NdH3eQ2F2VlZX4dm5+fH+x2gLDYuXNnbbfgOdwZAADAcoQBAAAsRxgAAMByhAEAACzn2Q2EDz74oGs9IiLCqN1xxx2hbqfWFRUVGbUxY8a4rt2+fXuo20Ed0aJFC6P2u9/9zq9jhwwZYtRef/31gHsCfsrta95rYuLEiUHqxG7cGQAAwHKEAQAALEcYAADAcoQBAAAsRxgAAMBynv00QWlpqWs9IyPDqL3xxhtGbfHixUYtNjY28MaC7IcffjBqTz/9tFFbunSpUduxY0dIeoJ3FBcXG7V3333XqPXu3duo8ckBhENNHs/+/fffGzW3T1Kh5rgzAACA5QgDAABYjjAAAIDlCAMAAFjOsxsIq3Lq1Cmjlp2dbdQ+//xzo3b11Vf7fZ24uDijNm3aNL+Pd/O3v/3NqP33v/81akuWLAnoOrBHWVmZUXP7rne3TbZAONTkccR5eXlGbdWqVcFsx1rcGQAAwHKEAQAALEcYAADAcoQBAAAs53P8ePxTSUlJnXw6H7ypuLhYMTExYbkWs4tgYnbhVdXNLncGAACwHGEAAADLEQYAALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwHGEAAADLEQYAALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwHGEAAADLEQYAALAcYQAAAMsRBgAAsBxhAAAAyxEGAACwnF9hwHGcUPcBi4RznphdBBOzC6+qbp78CgOlpaVBaQaQwjtPzC6CidmFV1U3Tz7Hj/hZXl6uoqIiRUdHy+fzBa052MVxHJWWliopKUkNGoTnHSpmF8HA7MKr/J1dv8IAAACov9hACACA5QgDAABYjjAAAIDlCAMAAFiOMAAAgOUIAwAAWI4wAACA5f4fQc3jHfY2RzUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 13\u001b[0m\n\u001b[1;32m      1\u001b[0m layers \u001b[38;5;241m=\u001b[39m [ConvolutionalLayer(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;66;03m# (28 - 3 + 2 * 1) / 1 + 1 = 28\u001b[39;00m\n\u001b[1;32m      2\u001b[0m           MaxPoolLayer(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;66;03m# (28 - 3 + 2 * 2) / 2 + 1 = 15\u001b[39;00m\n\u001b[1;32m      3\u001b[0m           ConvolutionalLayer(input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m15\u001b[39m), output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m), \u001b[38;5;66;03m# (15 - 3 + 2 * 3) / 3 + 1 = 7\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m           DenseLayer(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m      9\u001b[0m           Softmax()]\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(layers\u001b[38;5;241m=\u001b[39mlayers, loss_function\u001b[38;5;241m=\u001b[39mCCE_Loss(), optimizer\u001b[38;5;241m=\u001b[39mOptimizer_SGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-3\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m))\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programstvo/Python/Strojno/DLFS/dlfs/model.py:118\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, X, y, epochs, print_every)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03mTrain the model.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_every \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_every:\n",
      "File \u001b[0;32m~/Desktop/Programstvo/Python/Strojno/DLFS/dlfs/model.py:48\u001b[0m, in \u001b[0;36mModel._forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Forward data through all the layers\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m1\u001b[39m:], start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 48\u001b[0m     \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Output of the model is the output of the last layer\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput\n",
      "Cell \u001b[0;32mIn[27], line 77\u001b[0m, in \u001b[0;36mMaxPoolLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     74\u001b[0m axis_1_end \u001b[38;5;241m=\u001b[39m axis_1_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding:\n\u001b[0;32m---> 77\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs[i, j]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Programi/Python_venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:805\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    804\u001b[0m     values \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 805\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_as_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, width_pair, value_pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(axes, pad_width, values):\n\u001b[1;32m    807\u001b[0m         roi \u001b[38;5;241m=\u001b[39m _view_roi(padded, original_area_slice, axis)\n",
      "File \u001b[0;32m~/Programi/Python_venv/lib/python3.10/site-packages/numpy/lib/arraypad.py:458\u001b[0m, in \u001b[0;36m_as_pairs\u001b[0;34m(x, ndim, as_index)\u001b[0m\n\u001b[1;32m    453\u001b[0m         padded[pad_area] \u001b[38;5;241m=\u001b[39m left_chunk\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_left_pad, new_right_pad\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_pairs\u001b[39m(x, ndim, as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    Broadcast `x` to an array with the shape (`ndim`, 2).\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m        Or if `x` is not broadcastable to the shape (`ndim`, 2).\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;66;03m# Pass through None as a special case, otherwise np.round(x) fails\u001b[39;00m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;66;03m# with an AttributeError\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=1, padding=1), # (28 - 3 + 2 * 1) / 1 + 1 = 28\n",
    "          MaxPoolLayer(input_shape=(3, 28, 28), kernel_size=3, stride=2, padding=2), # (28 - 3 + 2 * 2) / 2 + 1 = 15\n",
    "          ConvolutionalLayer(input_shape=(3, 15, 15), output_channels=4, kernel_size=3, stride=3, padding=3), # (15 - 3 + 2 * 3) / 3 + 1 = 7\n",
    "          MaxPoolLayer(input_shape=(4, 7, 7), kernel_size=3, stride=2, padding=1),  # (7 - 3 + 2 * 1) / 2 + 1 = 4\n",
    "          ReshapeLayer(input_shape=(4, 4, 4), output_shape=4*4*4),\n",
    "          DenseLayer(4*4*4, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
