{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $X$\n",
    "\n",
    "- kernel matrix $k$\n",
    "\n",
    "- output matrix $Y$\n",
    "\n",
    "$$Y = X \\star_{\\text{valid}} k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to input matrix $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to kernel $\\frac{\\partial L}{\\partial k}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\delta \\ast_{\\text{full}} k \\quad \\quad \\frac{\\partial L}{\\partial k} = X \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "        output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_width, output_height)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "            \n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            # Loop through output and input channels\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=False)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                            # If padding is present dinput should be unpadded\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "                            # If padding is present dinput should be unpadded\n",
    "                            dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        if self.stride == 1:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=False)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=False)\n",
    "                        else:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], stride=True)\n",
    "                            dinputs = self._calculate_input_gradient(delta[i, j], self.kernels[j, k], stride=True)\n",
    "\n",
    "                    # Update gradients for the current sample, output and input channel\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, stride: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating kernel gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Current accumulated gradient.\n",
    "\n",
    "        stride : bool, default=False\n",
    "            Checks whether stride is present.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dkernel : np.ndarray\n",
    "            Kernel gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride:\n",
    "\n",
    "            # If stride is present delta must be dilated\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            delta_dilated_shape = delta_dilated.shape\n",
    "\n",
    "            # Get input and kernel shape (assumes they are square matrices)\n",
    "            input_shape = self.inputs.shape[-1]\n",
    "            kernel_shape = self.kernels_shape[-1]\n",
    "            padding = self.padding\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 2 * padding + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient is computed\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_shape - self.kernel_size + 2 * self.padding + 1, input_shape - kernel_shape + 2*self.padding + 1)\n",
    "                delta_dilated = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "        else:\n",
    "            # Gradient with respect to kernel is valid cross correlation between input and delta\n",
    "            dkernels = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernels\n",
    "\n",
    "    def _calculate_input_gradient(self, delta: np.ndarray, kernel: np.ndarray, stride: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating input gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Current accumulated gradient.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Current kernel.\n",
    "\n",
    "        stride : bool, default=False\n",
    "            Checks whether stride is present.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dinputs : np.ndarray\n",
    "           Input gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride:\n",
    "            # If stride is present delta must be dilated\n",
    "            delta_dilated = dilate(delta, stride=self.stride)\n",
    "            dinputs = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "        else:\n",
    "            # Gradient with respect to input is a full convolution between delta and kernel\n",
    "            dinputs = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int, int], output_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple[int, int, int]\n",
    "            Input shape of a single sample. For images it's (channels, width, height).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels, width, height) to (batch_size, channels * width * height). Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels * width * height) to (batch_size, channels, width, height). Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        self.output_height = int(floor((input_height - kernel_size + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # List for storing indices of max elements\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_width):\n",
    "                    for l in range(self.output_height):\n",
    "                        \n",
    "                        # Initalize axis 0 start and end indices \n",
    "                        axis_0_start = k * self.stride\n",
    "                        axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "\n",
    "                        if self.padding:\n",
    "                            arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                        else:\n",
    "                            arr = self.inputs[i, j].copy()\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "        #print(f'output maxpool: {self.output.shape}')\n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        # Initialize input gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                dinput = np.zeros((input_shape[2] + 2 * self.padding, input_shape[3] + 2 * self.padding))\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()\n",
    "                \n",
    "\n",
    "        #print(f'u backwardu maxpool layera dinput: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.6929092482862151 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.5008076454337138 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.21556318013589224 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.07383741775822975 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.0490629522209486 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=2, stride=3, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 10, 10), kernel_size=3, stride=2, padding=0), \n",
    "          ReshapeLayer(input_shape=(3, 4, 4), output_shape=3*4*4),\n",
    "          DenseLayer(3*4*4, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2448.576610368768 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2364.2192638097113 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2293.6540196134174 =====\n",
      "===== EPOCH : 15 ===== LOSS : 2288.8576595675486 =====\n",
      "===== EPOCH : 20 ===== LOSS : 2291.0847013048988 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 14, 14), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 8, 8), output_shape=3*8*8),\n",
    "          DenseLayer(3*8*8, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
