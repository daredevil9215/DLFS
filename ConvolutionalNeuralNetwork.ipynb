{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $X$\n",
    "\n",
    "- kernel matrix $k$\n",
    "\n",
    "- output matrix $Y$\n",
    "\n",
    "$$Y = X \\star_{\\text{valid}} k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to input matrix $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to kernel $\\frac{\\partial L}{\\partial k}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\delta \\ast_{\\text{full}} k \\quad \\quad \\frac{\\partial L}{\\partial k} = X \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        output_height = int(floor((input_height - kernel_size + + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_width, output_height)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "            \n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                        dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                            # Gradient with respect to inputs is the full convolution between delta and kernel\n",
    "                            dinputs = self._calculate_input_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1):\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "            delta_dilated_shape = delta_dilated.shape[-1]\n",
    "\n",
    "            input_shape = inputs.shape[-1]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient is computed\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_shape - kernel_shape + 1, input_shape - kernel_shape + 1)\n",
    "                delta_dilated = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernels = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "\n",
    "        else:\n",
    "            dkernels = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernels\n",
    "\n",
    "    def _calculate_input_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1):\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "            delta_dilated_shape = delta_dilated.shape[-1]\n",
    "\n",
    "            input_shape = inputs.shape[-1]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 1:\n",
    "                dinputs = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "            else:\n",
    "                new_delta_shape = (input_shape - kernel_shape + 1, input_shape - kernel_shape + 1)\n",
    "                delta_dilated = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dinputs = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "\n",
    "        else:\n",
    "            dinputs = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int, int], output_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple[int, int, int]\n",
    "            Input shape of a single sample. For images it's (channels, width, height).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels, width, height) to (batch_size, channels * width * height). Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels * width * height) to (batch_size, channels, width, height). Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_width = int(floor((input_width - kernel_size + 2 * padding) / stride) + 1)\n",
    "        self.output_height = int(floor((input_height - kernel_size + 2 * padding) / stride) + 1) \n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # List for storing indices of max elements\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_width):\n",
    "                    for l in range(self.output_height):\n",
    "                        \n",
    "                        # Initalize axis 0 start and end indices \n",
    "                        axis_0_start = k * self.stride\n",
    "                        axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "\n",
    "                        if self.padding:\n",
    "                            arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                        else:\n",
    "                            arr = self.inputs[i, j].copy()\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "        #print(f'output maxpool: {self.output.shape}')\n",
    "\n",
    "    def backward(self, delta):\n",
    "\n",
    "        # Initialize input gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                dinput = np.zeros((input_shape[2] + 2 * self.padding, input_shape[3] + 2 * self.padding))\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()\n",
    "                \n",
    "\n",
    "        #print(f'u backwardu maxpool layera dinput: {self.dinputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 00:01:36.811142: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-20 00:01:36.815797: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-20 00:01:36.832430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-20 00:01:36.864888: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-20 00:01:36.864934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-20 00:01:36.884637: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-20 00:01:38.175623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.7054187544321731 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.6065982657448827 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.42215977189340137 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.2409386757917405 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.1491330765236072 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=2, stride=3, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 10, 10), kernel_size=3, stride=2, padding=0), \n",
    "          ReshapeLayer(input_shape=(3, 4, 4), output_shape=3*4*4),\n",
    "          DenseLayer(3*4*4, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2451.8810289068097 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2380.0628487877657 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2311.5898874300924 =====\n",
      "===== EPOCH : 15 ===== LOSS : 2294.2015054580334 =====\n",
      "===== EPOCH : 20 ===== LOSS : 2289.310077040057 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 14, 14), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 8, 8), output_shape=3*8*8),\n",
    "          DenseLayer(3*8*8, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGFCAYAAABg2vAPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjrUlEQVR4nO3de1hVZfr/8XuDB8yQ1ERDDSQ1ZdIGNXMQC7Oy0klKs7KrcjpejjM1jpodVJxqMk3LMSudMVPTxmkUGkfNjmhTEeYxTfGUeCIVQQEzMdzr98d8h5/2PMgCNhv2vt+v6/KPPrPW2jd9n299WjxrbY/jOI4AAAC1Qmp6AAAAULMoAwAAKEcZAABAOcoAAADKUQYAAFCOMgAAgHKUAQAAlKMMAACgHGUAAADlKANlyM7OFo/HI1OmTPHZNVetWiUej0dWrVrls2sCP8faRaBi7dacoCoDc+fOFY/HI2vXrq3pUfzihhtuEI/HI7/73e9qehRUUbCv3ZiYGPF4PNY/7dq1q+nxUAXBvna3b98uI0aMkISEBAkLCxOPxyPZ2dk1PZbP1anpAVA5qampkpGRUdNjAK5MmzZNTpw4cU62d+9eGTt2rNx44401NBVQvoyMDJk+fbrExcVJx44dZePGjTU9UrWgDASgU6dOyciRI2XMmDEyfvz4mh4HKFdycrKRPf/88yIics899/h5GsC9W2+9VY4fPy7h4eEyZcqUoC0DQfVrAjdOnz4t48ePl65du0pERIQ0bNhQevXqJenp6WWe88orr0h0dLQ0aNBArr32WtmyZYtxTFZWlgwaNEiaNGkiYWFh0q1bN1m6dGm585w8eVKysrLk6NGjrn+GyZMni9frlVGjRrk+B4EvGNbu2d555x1p06aNJCQkVOp8BI5AXrtNmjSR8PDwco8LdOrKQGFhocyePVuSkpJk0qRJMmHCBMnNzZW+fftaG9/8+fNl+vTpMnz4cHnqqadky5Ytct1118nhw4dLj/n222+lR48esm3bNnnyySdl6tSp0rBhQ0lOTpa0tLTzzrNmzRrp2LGjzJgxw9X8+/btkxdffFEmTZokDRo0qNDPjsAW6Gv3bBs2bJBt27bJkCFDKnwuAk8wrd2g5QSRt956yxER5+uvvy7zmJKSEqe4uPic7NixY07z5s2dBx54oDTbs2ePIyJOgwYNnAMHDpTmmZmZjog4I0aMKM369OnjdOrUyTl16lRp5vV6nYSEBKddu3alWXp6uiMiTnp6upGlpKS4+hkHDRrkJCQklP61iDjDhw93dS5qLw1r92wjR450RMTZunVrhc9F7aJp7b700kuOiDh79uyp0HmBQN2dgdDQUKlXr56IiHi9XsnPz5eSkhLp1q2brF+/3jg+OTlZWrZsWfrX3bt3l6uvvlpWrFghIiL5+fny6aefyuDBg6WoqEiOHj0qR48elby8POnbt6/s3LlTDh48WOY8SUlJ4jiOTJgwodzZ09PTZcmSJTJt2rSK/dAICoG8ds/m9Xpl0aJFEh8fLx07dqzQuQhMwbJ2g5m6MiAiMm/ePOncubOEhYVJ06ZNpVmzZrJ8+XIpKCgwjrU99tS+ffvSR0t27doljuPIuHHjpFmzZuf8SUlJERGRI0eOVHnmkpISeeyxx+Tee++Vq666qsrXQ2AKxLX7c6tXr5aDBw+ycVCZYFi7wUzd0wQLFiyQoUOHSnJysowePVoiIyMlNDRUJk6cKLt3767w9bxer4iIjBo1Svr27Ws9pm3btlWaWeS/v0Pbvn27zJo1y3jGtaioSLKzsyUyMlIuuOCCKn8WaqdAXbs/t3DhQgkJCZG7777b59dG7RQsazeYqSsDixcvltjYWElNTRWPx1Oa/69N/tzOnTuNbMeOHRITEyMiIrGxsSIiUrduXbn++ut9P/D/2bdvn/z000/Ss2dP43+bP3++zJ8/X9LS0qyPcCE4BOraPVtxcbEsWbJEkpKSJCoqyi+fiZoXDGs32Kn7NUFoaKiIiDiOU5plZmaW+QKf995775zfPa1Zs0YyMzPl5ptvFhGRyMhISUpKklmzZsn3339vnJ+bm3veedw+4nLXXXdJWlqa8UdE5JZbbpG0tDS5+uqrz3sNBLZAXbtnW7FihRw/fpxfESgTDGs32AXlnYE5c+bIypUrjfzxxx+X/v37S2pqqtx2223Sr18/2bNnj8ycOVPi4uKMN6SJ/PdWU2JiogwbNkyKi4tl2rRp0rRpU3niiSdKj3nttdckMTFROnXqJA8//LDExsbK4cOHJSMjQw4cOCCbNm0qc9Y1a9ZI7969JSUl5bybWTp06CAdOnSw/m9t2rThjkCQCMa1e7aFCxdK/fr1ZeDAga6OR+AI1rVbUFAgr776qoiIfPHFFyIiMmPGDLnooovkoosuCp7XwdfYcwzV4H+PuJT1Z//+/Y7X63VeeOEFJzo62qlfv74THx/vLFu2zLn//vud6Ojo0mv97xGXl156yZk6darTunVrp379+k6vXr2cTZs2GZ+9e/du57777nNatGjh1K1b12nZsqXTv39/Z/HixaXH+PrxLMfh0cJgoWHtFhQUOGFhYc7tt99e2b9NqIWCfe3+bybbn7NnD3Qexznrvg0AAFBH3Z4BAABwLsoAAADKUQYAAFCOMgAAgHKUAQAAlKMMAACgnKuXDnm9XsnJyZHw8PBzXiUJVITjOFJUVCRRUVESEuKfHsrahS+wdhGo3K5dV2UgJydHWrdu7bPhoNv+/fulVatWfvks1i58ibWLQFXe2nVVccPDw302EODP9cTahS+xdhGoyltPrsoAt6jgS/5cT6xd+BJrF4GqvPXEBkIAAJSjDAAAoBxlAAAA5SgDAAAoRxkAAEA5ygAAAMpRBgAAUI4yAACAcpQBAACUowwAAKAcZQAAAOUoAwAAKEcZAABAOcoAAADKUQYAAFCOMgAAgHKUAQAAlKtT0wMAAFBZvXv3NrI333zTyK699lrr+fv37/f5TIGIOwMAAChHGQAAQDnKAAAAylEGAABQjg2ElXTppZca2fDhw41swIABRnb55Zdbr3ny5Ekj69Wrl5GtX7/ezYgAEFSio6ONbM6cOa6Oe+CBB6zXnDx5spH9+OOPlZgusHFnAAAA5SgDAAAoRxkAAEA5ygAAAMqxgfAsderY/3bcdNNNRjZx4kQji4uLc/U5Xq/XmoeFhRlZ27ZtjYwNhAA0io2NNTLbZkGblJQUa965c2cjGzhwYMUGCwLcGQAAQDnKAAAAylEGAABQjjIAAIByajcQ2t4CuHjxYuuxbjcGulXWBsKQELObderUycjeffddI4uIiDCy8ePHWz/ntttuM7KOHTsaWXFxsfV8uDdo0CAje/jhh63H5uTkGNmpU6eMbOHChUZ26NAh6zV37dpV3ohAwBg1alRNjxC0uDMAAIBylAEAAJSjDAAAoBxlAAAA5SgDAAAop+JpgqioKCP7+OOPXR1XlsLCQiNLTU01sszMTCP79ttvrdd85JFHjGzz5s1G1rNnTyN79dVXjezKK6+0fo7NXXfdZWTz5s1zfT7sbN+VHhMTU6VrPvroo0ZWVFRkPbastVabHDhwwMhsf99ERNauXVvd40CZgoKCmh6hVuDOAAAAylEGAABQjjIAAIBylAEAAJQLug2Etlf6Pvfcc0Zm2yx45swZ6zWXLl1qZDNnzjQy26bEivj666+NbOjQoUY2YcIEI7O9Xrksn3zyiZEtWLDA9flwz/bqYdv3p4uIbNu2zchsr4nu0qWLkSUlJVmv2aNHDyPbv3+/kbVu3dp6vlslJSVGlpuba2SXXHKJq+vt27fPmrOBEJX1ww8/WPMpU6b4eZLaiTsDAAAoRxkAAEA5ygAAAMpRBgAAUC7oNhC+8cYbRmbbhGfbLPjQQw9Zrzl//vwqz3W2OnXsf9tHjx5tZE8//bSRhYWFGdmxY8eMLCUlxfo5f/3rX42srM2TqBrbZk1bVpaVK1e6Oq5x48bW/Je//KWRrVu3zsiuuuoq1zPZnDp1ysh27NhhZLZNkk2aNDGy3bt3V2keBL7Y2Fgji4+Pr/T1Vq1aZc23bt1a6WsGE+4MAACgHGUAAADlKAMAAChHGQAAQLmg20B4ww03uDrO9lZBX28UFLFvgpkzZ4712F69erm6Zn5+vpH17dvXyNavX+/qegh8tg2kIiLp6emuzq/Ipka3Bg4caGS2jY62r+n+xz/+4fN5EFhsb+9s0aJFpa936NChqowT9LgzAACAcpQBAACUowwAAKAcZQAAAOUoAwAAKBd0TxO4tWTJEp9f8xe/+IWRjRs3zsjcPjUgIvLll18ame21xTw5gJoUGRlpZK+//rqRhYSY//3x7LPPGpntiRkEpwsuuMCa/+pXv/Lp58yePdun1ws23BkAAEA5ygAAAMpRBgAAUI4yAACAcmo3EA4YMMDI3nvvPeuxP/74o5HVq1fPyF588UUju+WWW1zPZPtO+JdfftnIvvrqK9fXBPxh+PDhRtasWTMjs702efv27dUyEwKD7ZXtIiLXXHONnyfRjTsDAAAoRxkAAEA5ygAAAMpRBgAAUC7oNhAePnzYyKKjo43sjjvuMLL27dtbrzls2DAjGzFihJG53SyYl5dnzW+++WYjW7dunatrAv7Qs2dPa/7kk0+6Oj85OdnItmzZUpWRAIPtDZYFBQU1MEng4M4AAADKUQYAAFCOMgAAgHKUAQAAlAu6DYSDBw82suzsbFfnXnnlldbc9jXCbqWmphrZmDFjrMd+9913lf4cwB/K2iRbt25dI/vkk0+MLCMjw+czAT9n+0r3rKysGpgkcHBnAAAA5SgDAAAoRxkAAEA5ygAAAMoF3QbCAwcOGNmgQYOM7JlnnjGysjYQhoRUvjN9+OGHRsZGQQSCBg0aGNlNN91kPfb06dNGlpKSYmQ//fRT1QcDyjF79uyaHiHgcGcAAADlKAMAAChHGQAAQDnKAAAAylEGAABQLuieJnAcx8jS0tKM7ODBg0aWnp5uvWZYWFil53nssceM7LPPPrMeu3379kp/DuBro0ePNrL4+HjrsStXrjSyqrzGG3Br7dq1RrZ8+fIamCSwcWcAAADlKAMAAChHGQAAQDnKAAAAygXdBkK3WrZsaWRlbRT8/PPPjaykpMTIkpKSjCwuLs7IvvjiC+vntGnTxsiKioqsxwK+1K9fPyMbN26ckRUWFlrPf/bZZ30+E3TIzc215rYN1ZdffrmRXXHFFUZ21113Gdmbb75Zien04M4AAADKUQYAAFCOMgAAgHKUAQAAlFOxgbBFixZGNnnyZCM7efKk9fw+ffq4+py33nrLyIYMGWJkjRs3tp6fmJhoZO+//76rzwbcatq0qZFNnz7dyEJDQ41sxYoV1mt+9dVXVR8MKtWpY//XkNs3v9qOu/POO42MDYTnx50BAACUowwAAKAcZQAAAOUoAwAAKKdiA+HFF19sZLGxsUZW1gZC29sGbR588EEj27x5s5FNnDjRer7tLW7r1q0zsiNHjriaB7BtArR93bDt7Ze7d+82MttbCYGqOHXqlDUvKCjw8yS6cWcAAADlKAMAAChHGQAAQDnKAAAAyqnYQJifn29kOTk5RmbbaCgi0qFDByPLysoystOnTxtZXl6emxFFRCQmJsbIiouLXZ8P/Nxll11mZF27dnV17h//+Ecjs20qBKqirDcQ1q9f38+T6MadAQAAlKMMAACgHGUAAADlKAMAAChHGQAAQDkVTxPYnhyYM2eOkY0dO9Z6vu2Vwl6v18i++eYbI4uPj3czooiIHDt2zMh4JSfciI6OtuYffvihq/NHjx5tZMuWLavSTIAbHo/HmoeE8N+q/sTfbQAAlKMMAACgHGUAAADlKAMAACinYgOhzRtvvGFknTt3th576623Gpltc0uXLl2qNNPgwYOrdD70euSRR6z5pZde6ur81atXG5njOFWaCXDj0KFD1nzmzJlG9vLLL7u65n/+858qzaQRdwYAAFCOMgAAgHKUAQAAlKMMAACgnMdxsUuosLBQIiIi/DFPjSrr+7MnT55sZLfffruRRUVFGdnhw4eNrKyNghkZGUZ25swZ67GBrKCgQBo1auSXzwrGtZuYmGhkK1assB574YUXurpm9+7djWzt2rUVG0wB1i4CVXlrlzsDAAAoRxkAAEA5ygAAAMpRBgAAUE7tGwhtiouLrfnjjz/uKgP8oVevXkbmdqOgiMju3buN7MSJE1WaCUBg484AAADKUQYAAFCOMgAAgHKUAQAAlKMMAACgHE8TAEFs06ZNRtanTx8jy8/P98c4AGop7gwAAKAcZQAAAOUoAwAAKEcZAABAOY/jOE55B/G92vAlvhMegYq1i0BV3trlzgAAAMpRBgAAUI4yAACAcq7KgIttBYBr/lxPrF34EmsXgaq89eSqDBQVFflkGEDEv+uJtQtfYu0iUJW3nlw9TeD1eiUnJ0fCw8PF4/H4bDjo4jiOFBUVSVRUlISE+Oc3VKxd+AJrF4HK7dp1VQYAAEDwYgMhAADKUQYAAFCOMgAAgHKUAQAAlKMMAACgHGUAAADlKAMAAChHGQAAQDnKAAAAylEGAABQjjIAAIBylAEAAJSjDAAAoBxlAAAA5SgDAAAoRxkAAEA5ygAAAMpRBgAAUI4yAACAcpQBAACUowwAAKAcZQAAAOUoAwAAKEcZAABAOcoAAADKUQYAAFCOMgAAgHKUAQAAlKMMAACgHGUAAADlKAMAAChHGQAAQDnKQBmys7PF4/HIlClTfHbNVatWicfjkVWrVvnsmsDPsXYRqFi7NSeoysDcuXPF4/HI2rVra3qUahETEyMej8f6p127djU9HqqAtYtAxdoNDnVqegC4N23aNDlx4sQ52d69e2Xs2LFy44031tBUQPlYuwhUWtYuZSCAJCcnG9nzzz8vIiL33HOPn6cB3GPtIlBpWbtB9WsCN06fPi3jx4+Xrl27SkREhDRs2FB69eol6enpZZ7zyiuvSHR0tDRo0ECuvfZa2bJli3FMVlaWDBo0SJo0aSJhYWHSrVs3Wbp0abnznDx5UrKysuTo0aOV+nneeecdadOmjSQkJFTqfAQO1i4CFWu39lNXBgoLC2X27NmSlJQkkyZNkgkTJkhubq707dtXNm7caBw/f/58mT59ugwfPlyeeuop2bJli1x33XVy+PDh0mO+/fZb6dGjh2zbtk2efPJJmTp1qjRs2FCSk5MlLS3tvPOsWbNGOnbsKDNmzKjwz7JhwwbZtm2bDBkypMLnIvCwdhGoWLsBwAkib731liMiztdff13mMSUlJU5xcfE52bFjx5zmzZs7DzzwQGm2Z88eR0ScBg0aOAcOHCjNMzMzHRFxRowYUZr16dPH6dSpk3Pq1KnSzOv1OgkJCU67du1Ks/T0dEdEnPT0dCNLSUmp8M87cuRIR0ScrVu3Vvhc1C6sXQQq1m5wUHdnIDQ0VOrVqyciIl6vV/Lz86WkpES6desm69evN45PTk6Wli1blv519+7d5eqrr5YVK1aIiEh+fr58+umnMnjwYCkqKpKjR4/K0aNHJS8vT/r27Ss7d+6UgwcPljlPUlKSOI4jEyZMqNDP4fV6ZdGiRRIfHy8dO3as0LkITKxdBCrWbu2nrgyIiMybN086d+4sYWFh0rRpU2nWrJksX75cCgoKjGNtj460b99esrOzRURk165d4jiOjBs3Tpo1a3bOn5SUFBEROXLkiM9/htWrV8vBgweDagMLysfaRaBi7dZu6p4mWLBggQwdOlSSk5Nl9OjREhkZKaGhoTJx4kTZvXt3ha/n9XpFRGTUqFHSt29f6zFt27at0sw2CxculJCQELn77rt9fm3UTqxdBCrWbu2nrgwsXrxYYmNjJTU1VTweT2n+vzb5czt37jSyHTt2SExMjIiIxMbGiohI3bp15frrr/f9wBbFxcWyZMkSSUpKkqioKL98JmoeaxeBirVb+6n7NUFoaKiIiDiOU5plZmZKRkaG9fj33nvvnN89rVmzRjIzM+Xmm28WEZHIyEhJSkqSWbNmyffff2+cn5ube955KvOIy4oVK+T48eNBeasKZWPtIlCxdmu/oLwzMGfOHFm5cqWRP/7449K/f39JTU2V2267Tfr16yd79uyRmTNnSlxcnPGWKZH/3mpKTEyUYcOGSXFxsUybNk2aNm0qTzzxROkxr732miQmJkqnTp3k4YcfltjYWDl8+LBkZGTIgQMHZNOmTWXOumbNGundu7ekpKS43syycOFCqV+/vgwcONDV8QgcrF0EKtZuYAvKMvDGG29Y86FDh8rQoUPl0KFDMmvWLPnggw8kLi5OFixYIP/85z+tX2Rx3333SUhIiEybNk2OHDki3bt3lxkzZsgll1xSekxcXJysXbtW/vSnP8ncuXMlLy9PIiMjJT4+XsaPH+/Tn62wsFCWL18u/fr1k4iICJ9eGzWPtYtAxdoNbB7n7Ps2AABAHXV7BgAAwLkoAwAAKEcZAABAOcoAAADKUQYAAFDO1aOFXq9XcnJyJDw8/Jy3RwEV4TiOFBUVSVRUlISE+KeHsnbhC6xdBCq3a9dVGcjJyZHWrVv7bDjotn//fmnVqpVfPou1C19i7SJQlbd2XVXc8PBwnw0E+HM9sXbhS6xdBKry1pOrMsAtKviSP9cTaxe+xNpFoCpvPbGBEAAA5SgDAAAoRxkAAEA5ygAAAMpRBgAAUI4yAACAcpQBAACUowwAAKAcZQAAAOUoAwAAKEcZAABAOcoAAADKUQYAAFCOMgAAgHKUAQAAlKMMAACgHGUAAADlKAMAAChHGQAAQDnKAAAAylEGAABQjjIAAIBylAEAAJSrU9MD4PwWLVpkZP/+97+NbOHChf4YBwFm9+7dRvb6668b2dSpU/0xDoLUgw8+aGSzZ8/2+efs3LnT1ecsXbrUyLKysnw+TzDhzgAAAMpRBgAAUI4yAACAcpQBAACUYwNhLRISYnaz6667zsi2bt3qj3EQQLp27WrN27RpY2QxMTHVPM1/RUZGGtnGjRuNbO7cuUb29NNPV8NEqCrb5mURkRtuuMHIHMfx+ee3bdvWyF588UUja968uZGNHDnS5/MEE+4MAACgHGUAAADlKAMAAChHGQAAQDk2ENYi8fHxRnbxxRfXwCQINKNHj3Z9bHZ2dvUNchbbhljbxq4uXbr4YxxUUFJSkpHZNjSLiNSrV8/INm/ebGR79+51/fl//vOfjeyKK64wsr/97W9G9vvf/97INmzYYP2cBQsWuJ4pmHFnAAAA5SgDAAAoRxkAAEA5ygAAAMpRBgAAUE7F0wTt27c3silTphiZbQeqSMV2wPqDbZcu9LC9erisXd423333nS/HKVOfPn388jmoHrZ/zvzrX/+yHhseHm5kjzzyiJF9//33VZqpUaNGro6rU8f8V1vjxo2r9NnBjjsDAAAoRxkAAEA5ygAAAMpRBgAAUE7FBsIePXoYWf/+/Y1s3rx51vP9tYHQ9l3dNgcPHqzmSVBb2DZmvfvuu0ZW1mur58+fb2RpaWlVH8wF22uGPR6PkWVkZPhjHFRQXl6ekQ0ZMqQGJqm4kpISIyssLKyBSQIHdwYAAFCOMgAAgHKUAQAAlKMMAACgnIoNhG7fzlbTG/Nsb+w6fvy4ka1fv94P06A2eOihh4wsJibGyBzHsZ6/ePFiX4/kmu3/72xz+uuNiAgsts2zd9xxh6tzX3/9dSMra4M4/os7AwAAKEcZAABAOcoAAADKUQYAAFAu6DYQ2jad2L5K1fYWtzVr1lTLTG7VrVvXyLxer5HZ3q6FwBcREWFkY8aMcXXusGHDrPmyZcuqNJM/BMKMqD62N8SKiHzwwQdGZvvnu01mZmaVZtKIOwMAAChHGQAAQDnKAAAAylEGAABQjjIAAIByQfc0QVxcnJG1bNnSyGy7TW0796vDRRddZM07duxoZB999FE1T4PaYuvWrUbWrFkzI1u3bp2RLVmypFpmcis2NtbI2rRp4+rcY8eO+Xoc1AL16tUzMttTL5MmTXJ9vo3tNfIbNmxwdS7+P+4MAACgHGUAAADlKAMAAChHGQAAQLmg20CYmJjo6rjVq1dX8yRlu/POO61506ZNjeyzzz6r7nFQjerXr29kb7/9tvXYSy65xNU1H3roISPLy8ur2GA+ZntNbKNGjWpgEtSE6OhoI/vyyy+NzO0arwjbBvEVK1YY2dNPP209f9GiRT6fKRBxZwAAAOUoAwAAKEcZAABAOcoAAADKBewGQtvGLBGR3/72t0aWn59vZLaNLLNnz7Zes3nz5kbWsGFDI7vmmmus5/+cx+NxdZyISFhYmOtjUfs0btzYyAYOHOj6fMdxjOzjjz82Mtt3v4uITJkyxdXn5ObmGllOTo6rc0XsP5NtdgSnOnXMf5VUx2ZBt2JiYoxs4cKF1mPHjBljZL/5zW+MbOPGjVUdq1bjzgAAAMpRBgAAUI4yAACAcpQBAACU8zgudvkUFhZKRESEP+Zxrax5qvJ1qGV9hfG2bduMLDs7u9Kf06dPH2tu2yxYXFxsZI8++qiRzZ8/v9Lz+FtBQYHf3k5X02vX9hXEe/futR5r+79/VTfh2Tar2q5p20Boe4Oc7auKRURatGhhZLaffdmyZUY2YMAA6zVrI01rtyJatWplZEuXLq3SNZ977jkjKyoqcnXuU089ZWS9e/d2/dm2r0VOTk42MtvXiddW5a1d7gwAAKAcZQAAAOUoAwAAKEcZAABAuYDdQFjWm/k2bdpkZJGRkUb2wgsvGNm8efOs1zxy5EgFpzu/ffv2WXPbJpwTJ04Y2ebNm42sZ8+eVR/MT7Rvwho0aJA1//Wvf21kXbp0MbKKvJXysssuM7KqbEos6+2ZtmuePHnSyG6//XYj++ijjyo9j79pX7uBIiEhwcgee+wx67GDBw92dc39+/cbmW0z+K5du1xdz9/YQAgAAM6LMgAAgHKUAQAAlKMMAACgHGUAAADlAvZpgrLYdkvavms7Pz/fH+NIy5YtjSwrK8t6rG0X6v33329ktl3atXUHqw07sv3n3nvvNbL4+HhX527YsMHIbE88iNifkDh06JCRRUVFufrs2oq1G7hCQ0OteWpqqpGVtc5/LjEx0chsr/GuDXiaAAAAnBdlAAAA5SgDAAAoRxkAAEA5c2ddgCssLKzpEc5x0003GVnDhg2tx9q+6/2bb77x+UzQ4+2333aVuWXbMCVifx3xsWPHKv05gK+dOXPGmg8YMMDIbJsKk5OTjWzBggVGduONN1o/p7Zv8ubOAAAAylEGAABQjjIAAIBylAEAAJQLug2EtU3jxo1dH7tq1arqGwTwgYqsZ9uGWCAQfPjhh0Zm20AYExNjZJdffrn1mmwgBAAAtRplAAAA5SgDAAAoRxkAAEA5NhDWIsXFxTU9AnBeXbp0qekRAJ/q0KGDkT3zzDM1MEnN4s4AAADKUQYAAFCOMgAAgHKUAQAAlGMDIQAr29cVX3bZZdZjbV9hnJGR4fOZgJ+78MILjaxr165Gduutt1rPHzx4sJG1bNnS1WcXFBQYWV5enqtzaxvuDAAAoBxlAAAA5SgDAAAoRxkAAEA5ygAAAMrxNEE1S0hIMDKPx2M91vZazM8//9znMwFutGnTxshsTw2UlX/33Xc+nwnBx/bPPRGRkBDzv1X/8Ic/uDrf9iRMVe3atcvIxo4da2RfffWVzz/bH7gzAACAcpQBAACUowwAAKAcZQAAAOXYQFjNwsPDjaysTVjHjh2r7nEA17Zu3er62B07dhjZzp07fTkOarHQ0FAji4qKMrJnn33WyO69917rNW0bCKtDbm6ukY0bN87I/v73vxtZUVFRtcxUE7gzAACAcpQBAACUowwAAKAcZQAAAOXYQFjNVq5caWQ//PCD9dj333+/uscBXFu3bp2Rffzxx9Zj09LSjOzHH3/0+UyonWwbpe+8804ja9WqlZFVdaPgwYMHjewvf/mLkZ05c8Z6/iuvvFKlzw8W3BkAAEA5ygAAAMpRBgAAUI4yAACAch6nrNfhnaWwsFAiIiL8MQ8UKCgokEaNGvnls1i78CXWLgJVeWuXOwMAAChHGQAAQDnKAAAAylEGAABQjjIAAIBylAEAAJSjDAAAoBxlAAAA5SgDAAAoRxkAAEA5ygAAAMpRBgAAUI4yAACAcpQBAACUc1UGXHzLMeCaP9cTaxe+xNpFoCpvPbkqA0VFRT4ZBhDx73pi7cKXWLsIVOWtJ4/jon56vV7JycmR8PBw8Xg8PhsOujiOI0VFRRIVFSUhIf75DRVrF77A2kWgcrt2XZUBAAAQvNhACACAcpQBAACUowwAAKAcZQAAAOUoAwAAKEcZAABAOcoAAADK/T+kc7UfGf67VwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
