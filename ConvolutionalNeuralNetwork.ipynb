{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural network\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $X$\n",
    "\n",
    "- kernel matrix $k$\n",
    "\n",
    "- output matrix $Y$\n",
    "\n",
    "$$Y = X \\star_{\\text{valid}} k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to input matrix $\\frac{\\partial L}{\\partial X}$\n",
    "\n",
    "- gradient of the loss function $L$ with respect to kernel $\\frac{\\partial L}{\\partial k}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial X} = \\delta \\ast_{\\text{full}} k \\quad \\quad \\frac{\\partial L}{\\partial k} = X \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "        output_height = int((input_height - kernel_size + + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_width, output_height)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "            \n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                        # Since padding was used gradient needs to be unpadded to match shape\n",
    "                        dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                            dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                            dinputs = self._calculate_input_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating kernel gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        kernel_grad : np.ndarray\n",
    "            Kernel gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            # If stride is present delta needs to be dilated\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_shape = delta_dilated.shape[-1]\n",
    "            input_shape = inputs.shape[-1]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient can be computed\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_shape - kernel_shape + 1, input_shape - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated_padded, \"valid\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to kernel is valid cross correlation between inputs and delta\n",
    "            dkernel = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernel\n",
    "\n",
    "    def _calculate_input_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1):\n",
    "        \"\"\"\n",
    "        Helper method for calculating input gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_grad : np.ndarray\n",
    "            Input gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_shape = delta_dilated.shape[-1]\n",
    "            input_shape = inputs.shape[-1]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_shape == input_shape - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed coonvolution shape gradient can be computed\n",
    "                dinput = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed coonvolution shape padding is needed\n",
    "                new_delta_shape = (input_shape - kernel_shape + 1, input_shape - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dinput = signal.convolve2d(delta_dilated_padded, kernel, \"full\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to inputs is full convolution between delta and kernel\n",
    "            dinput = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int, int], output_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple[int, int, int]\n",
    "            Input shape of a single sample. For images it's (channels, width, height).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels, width, height) to (batch_size, channels * width * height). Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels * width * height) to (batch_size, channels, width, height). Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Maxpooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_width, input_height = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "        self.output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_width, self.output_height)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the maxpool layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # List for storing indices of max elements (used in backward pass)\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_width):\n",
    "                    for l in range(self.output_height):\n",
    "                        \n",
    "                        # Initalize axis 0 start and end indices \n",
    "                        axis_0_start = k * self.stride\n",
    "                        axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "\n",
    "                        if self.padding:\n",
    "                            arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                        else:\n",
    "                            arr = self.inputs[i, j].copy()\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the maxpool layer. Creates gradient attribute with respect to inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize inputs gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Initialize gradient for current sample\n",
    "                if self.padding:\n",
    "                    dinput_shape = (input_shape[2] + 2 * self.padding, input_shape[3] + 2 * self.padding)\n",
    "                else:\n",
    "                    dinput_shape = input_shape[-2:]\n",
    "                    \n",
    "                dinput = np.zeros(dinput_shape)\n",
    "\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.7099707546055665 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.5127374921914126 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.21938821596050262 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.06888085000971395 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.051770029032036785 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer, MaxPoolLayer, ReshapeLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=2, stride=3, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 10, 10), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 6, 6), output_shape=3*6*6),\n",
    "          DenseLayer(3*6*6, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.995\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2451.8810289068097 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2380.0628487877657 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2311.5898874300924 =====\n",
      "===== EPOCH : 15 ===== LOSS : 2294.2015054580334 =====\n",
      "===== EPOCH : 20 ===== LOSS : 2289.310077040057 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 14, 14), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 8, 8), output_shape=3*8*8),\n",
    "          DenseLayer(3*8*8, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
