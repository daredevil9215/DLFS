{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](../img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](../img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](../img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](../img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $\\mathbf{X}$\n",
    "\n",
    "- kernel matrix $\\mathbf{k}$\n",
    "\n",
    "- output matrix $\\mathbf{Y}$\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X} \\star_{\\text{valid}} \\mathbf{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $\\mathcal{L}$ with respect to input matrix $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}$\n",
    "\n",
    "- gradient of the loss function $\\mathcal{L}$ with respect to kernel $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{k}}$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} = \\delta \\ast_{\\text{full}} \\mathbf{k} \\quad \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{k}} = \\mathbf{X} \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilation\n",
    "\n",
    "- inserting zeroes between consecutive elements\n",
    "\n",
    "- used for pixel skipping, just like stride skips pixels\n",
    "\n",
    "- $\\text{stride} - 1$ zeroes are inserted between elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dilation](../img/conv_dilate.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_height, input_width = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "        output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_height, output_width)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                        # Since padding was used gradient needs to be unpadded to match shape\n",
    "                        dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating kernel gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        kernel_grad : np.ndarray\n",
    "            Kernel gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            # If stride is present delta needs to be dilated\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_height, delta_dilated_width = delta_dilated.shape[-2:]\n",
    "            input_height, input_width = inputs.shape[-2:]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_height == input_height - kernel_shape + 1 and delta_dilated_width == input_width - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient can be computed\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_height - kernel_shape + 1, input_width - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated_padded, \"valid\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to kernel is valid cross correlation between inputs and delta\n",
    "            dkernel = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernel\n",
    "\n",
    "    def _calculate_input_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1):\n",
    "        \"\"\"\n",
    "        Helper method for calculating input gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_grad : np.ndarray\n",
    "            Input gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_height, delta_dilated_width = delta_dilated.shape[-2:]\n",
    "            input_height, input_width = inputs.shape[-2:]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_height == input_height - kernel_shape + 1 and delta_dilated_width == input_width - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed coonvolution shape gradient can be computed\n",
    "                dinput = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed convolution shape padding is needed\n",
    "                new_delta_shape = (input_height - kernel_shape + 1, input_width - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dinput = signal.convolve2d(delta_dilated_padded, kernel, \"full\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to inputs is full convolution between delta and kernel\n",
    "            dinput = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple[int, int, int], output_shape: int) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple[int, int, int]\n",
    "            Input shape of a single sample. For images it's (channels, width, height).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels, width, height) to (batch_size, channels * width * height). Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array from (batch_size, channels * width * height) to (batch_size, channels, width, height). Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Maxpooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_height, input_width = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "        self.output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the maxpool layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # List for storing indices of max elements (used in backward pass)\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                if self.padding:\n",
    "                    arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                else:\n",
    "                    arr = self.inputs[i, j].copy()\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_height):\n",
    "\n",
    "                    # Initalize axis 0 start and end indices \n",
    "                    axis_0_start = k * self.stride\n",
    "                    axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                    for l in range(self.output_width):\n",
    "                        \n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the maxpool layer. Creates gradient attribute with respect to inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize inputs gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        if self.padding:\n",
    "            dinput_height, dinput_width = input_shape[-2:]\n",
    "            dinput_shape = (dinput_height + 2 * self.padding, dinput_width + 2 * self.padding)\n",
    "        else:\n",
    "            dinput_shape = input_shape[-2:]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                \n",
    "                # Initialize gradient for current sample\n",
    "                dinput = np.zeros(dinput_shape)\n",
    "\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-21 16:58:32.614319: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 16:58:32.617148: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-21 16:58:32.626132: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-21 16:58:32.646175: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-21 16:58:32.646210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-21 16:58:32.658857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-21 16:58:33.517723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 100)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.842381852459142 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.5383396516748707 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.2098578993008045 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.03347177017302789 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.010219451156553951 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer, MaxPoolLayer, ReshapeLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=2),\n",
    "          MaxPoolLayer(input_shape=(3, 27, 27), kernel_size=3), \n",
    "          ReshapeLayer(input_shape=(3, 25, 25), output_shape=3*25*25),\n",
    "          DenseLayer(3*25*25, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(x_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(200, 1, 28, 28)\n",
      "(1000, 10)\n",
      "(200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = preprocess_whole_mnist(x_train[:1000])\n",
    "x_test = preprocess_whole_mnist(x_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2451.8810289068097 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2380.0628487877657 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2311.5898874300924 =====\n",
      "===== EPOCH : 15 ===== LOSS : 2294.2015054580334 =====\n",
      "===== EPOCH : 20 ===== LOSS : 2289.310077040057 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 14, 14), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 8, 8), output_shape=3*8*8),\n",
    "          DenseLayer(3*8*8, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_SGD(learning_rate=5e-3, momentum=0.9, decay=1e-2))\n",
    "\n",
    "model.train(x_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(x_test)\n",
    "\n",
    "for idx, x in enumerate(x_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
