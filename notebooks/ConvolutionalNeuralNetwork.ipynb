{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](../img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](../img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](../img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](../img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $\\mathbf{X}$\n",
    "\n",
    "- kernel matrix $\\mathbf{k}$\n",
    "\n",
    "- output matrix $\\mathbf{Y}$\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X} \\star_{\\text{valid}} \\mathbf{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $\\mathcal{L}$ with respect to input matrix $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}$\n",
    "\n",
    "- gradient of the loss function $\\mathcal{L}$ with respect to kernel $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{k}}$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} = \\delta \\ast_{\\text{full}} \\mathbf{k} \\quad \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{k}} = \\mathbf{X} \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilation\n",
    "\n",
    "- inserting zeroes between consecutive elements\n",
    "\n",
    "- used for pixel skipping, just like stride skips pixels\n",
    "\n",
    "- $\\text{stride} - 1$ zeroes are inserted between elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dilation](../img/conv_dilate.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_height, input_width = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "        output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_height, output_width)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                        # Since padding was used gradient needs to be unpadded to match shape\n",
    "                        dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating kernel gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        kernel_grad : np.ndarray\n",
    "            Kernel gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            # If stride is present delta needs to be dilated\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_height, delta_dilated_width = delta_dilated.shape[-2:]\n",
    "            input_height, input_width = inputs.shape[-2:]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_height == input_height - kernel_shape + 1 and delta_dilated_width == input_width - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient can be computed\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_height - kernel_shape + 1, input_width - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated_padded, \"valid\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to kernel is valid cross correlation between inputs and delta\n",
    "            dkernel = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernel\n",
    "\n",
    "    def _calculate_input_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1):\n",
    "        \"\"\"\n",
    "        Helper method for calculating input gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_grad : np.ndarray\n",
    "            Input gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_height, delta_dilated_width = delta_dilated.shape[-2:]\n",
    "            input_height, input_width = inputs.shape[-2:]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_height == input_height - kernel_shape + 1 and delta_dilated_width == input_width - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed coonvolution shape gradient can be computed\n",
    "                dinput = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed convolution shape padding is needed\n",
    "                new_delta_shape = (input_height - kernel_shape + 1, input_width - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dinput = signal.convolve2d(delta_dilated_padded, kernel, \"full\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to inputs is full convolution between delta and kernel\n",
    "            dinput = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Input shape of a single sample. For images it's (channels, height, width).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array to output shape. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, *self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array to input shape. Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Maxpooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_height, input_width = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "        self.output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the maxpool layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # List for storing indices of max elements (used in backward pass)\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                if self.padding:\n",
    "                    arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                else:\n",
    "                    arr = self.inputs[i, j].copy()\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_height):\n",
    "\n",
    "                    # Initalize axis 0 start and end indices \n",
    "                    axis_0_start = k * self.stride\n",
    "                    axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                    for l in range(self.output_width):\n",
    "                        \n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the maxpool layer. Creates gradient attribute with respect to inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize inputs gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        if self.padding:\n",
    "            dinput_height, dinput_width = input_shape[-2:]\n",
    "            dinput_shape = (dinput_height + 2 * self.padding, dinput_width + 2 * self.padding)\n",
    "        else:\n",
    "            dinput_shape = input_shape[-2:]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                \n",
    "                # Initialize gradient for current sample\n",
    "                dinput = np.zeros(dinput_shape)\n",
    "\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def load_mnist_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the header information\n",
    "        magic_number, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Read the image data\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "        return images\n",
    "    \n",
    "def load_mnist_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the header information\n",
    "        magic_number, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        # Read the label data\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "def load_mnist(path_train_data, path_train_labels, path_test_data, path_test_labels):\n",
    "    X_train = load_mnist_images(path_train_data)\n",
    "    y_train = load_mnist_labels(path_train_labels)\n",
    "\n",
    "    X_test = load_mnist_images(path_test_data)\n",
    "    y_test = load_mnist_labels(path_test_labels)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28), y_train: (60000,)\n",
      "X_test: (10000, 28, 28), y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "path_train_data = \"../mnist/train-images.idx3-ubyte\"\n",
    "path_train_labels = \"../mnist/train-labels.idx1-ubyte\"\n",
    "\n",
    "path_test_data = \"../mnist/t10k-images.idx3-ubyte\"\n",
    "path_test_labels = \"../mnist/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist(path_train_data, path_train_labels, path_test_data, path_test_labels)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (200, 1, 28, 28), y_train: (200,)\n",
      "X_test: (200, 1, 28, 28), y_test: (200,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = preprocess_data(X_train, y_train, 100)\n",
    "X_test, y_test = preprocess_data(X_test, y_test, 100)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.76695 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.61789 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.39467 =====\n",
      "===== EPOCH : 30 ===== LOSS : 0.16512 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer, MaxPoolLayer, ReshapeLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD, Optimizer_Adam\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=2, padding=1, stride=2),\n",
    "          MaxPoolLayer(input_shape=(3, 15, 15), kernel_size=3, stride=2), \n",
    "          ReshapeLayer(input_shape=(3, 7, 7), output_shape=(3*7*7, )),\n",
    "          DenseLayer(3*7*7, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_Adam(learning_rate=1e-2))\n",
    "\n",
    "model.train(X_train, y_train.reshape(-1, 1), print_every=10, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "for idx, x in enumerate(X_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1000, 1, 28, 28), y_train: (1000, 10)\n",
      "X_test: (200, 1, 28, 28), y_test: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist(path_train_data, path_train_labels, path_test_data, path_test_labels)\n",
    "\n",
    "X_train = preprocess_whole_mnist(X_train[:1000])\n",
    "X_test = preprocess_whole_mnist(X_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2411.83611 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2131.32714 =====\n",
      "===== EPOCH : 10 ===== LOSS : 1938.13058 =====\n",
      "===== EPOCH : 15 ===== LOSS : 1714.83507 =====\n",
      "===== EPOCH : 20 ===== LOSS : 1499.82863 =====\n"
     ]
    }
   ],
   "source": [
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 14, 14), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 8, 8), output_shape=(3*8*8, )),\n",
    "          DenseLayer(3*8*8, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_Adam(learning_rate=5e-3, decay=1e-2))\n",
    "\n",
    "model.train(X_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "for idx, x in enumerate(X_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
