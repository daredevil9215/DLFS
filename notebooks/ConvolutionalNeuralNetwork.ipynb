{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "- neural network for processing images (mostly)\n",
    "\n",
    "- consists of convolutional layers, maxpooling layers and standard dense, fully connected layers\n",
    "\n",
    "- idea is to scale down images using convolutional and maxpooling layers without losing too much information\n",
    "\n",
    "- once an image has been scaled and transformed to lower dimensions it can be passed to fully connected layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](../img/conv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "\n",
    "- operation from the field of digital signal processing\n",
    "\n",
    "- 2D convolution uses two matrices, input and kernel, to produce some output\n",
    "\n",
    "- a kernel matrix is slid over the input matrix, doing element-wise multiplication and summing\n",
    "\n",
    "- kernel can be thought of as a filter, and the result of the operation is a filtered image\n",
    "\n",
    "- depending on the kernel, there are many use cases: \n",
    "    - blurring\n",
    "    - smoothing\n",
    "    - edge detection\n",
    "    - sharpening\n",
    "    - feature detection\n",
    "    - noise reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ValidConvolution](../img/conv_valid.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FullConvolution](../img/conv_full.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid vs. full convolution\n",
    "\n",
    "- **valid**\n",
    "    - kernel is slid within borders of the input matrix\n",
    "    - kernel and input overlap completely\n",
    "    - output matrix is smaller in size compared to input matrix\n",
    "\n",
    "- **full**\n",
    "    - kernel is slid outside the borders of the input matrix\n",
    "    - kernel and input overlap partially at borders\n",
    "    - region outside of borders is padded with zeros\n",
    "    - output is larger in size compared to input matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Correlation vs. Convolution\n",
    "\n",
    "- Cross Correlation is sliding a kernel over the input matrix (denoted using $\\star$ symbol)\n",
    "\n",
    "- Convolution is sliding a *180 degrees rotated* kernel over the input matrix (denoted using $\\ast$ symbol)\n",
    "\n",
    "- this subtle difference is observed in backpropagation of the convolutional layer\n",
    "\n",
    "- Cross Correlation is used primarily in equations and code throughout this notebook, but the same can be achieved with Convolution with minor changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stride\n",
    "\n",
    "- step size of kernel when sliding over the input matrix\n",
    "\n",
    "- affects output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stride](../img/conv_stride.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output size formula (for square matrices)\n",
    "\n",
    "- $ \\text{valid} = \\lfloor \\frac{\\text{input size} - \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor + 1$\n",
    "\n",
    "- $\\text{full} = \\lfloor \\frac{\\text{input size} + \\text{kernel size} + 2 \\cdot \\text{padding}}{\\text{stride}} \\rfloor - 1$\n",
    "\n",
    "- $\\lfloor \\rfloor$ denotes the floor function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propagation for convolutional layer\n",
    "\n",
    "- input matrix $\\mathbf{X}$\n",
    "\n",
    "- kernel matrix $\\mathbf{k}$\n",
    "\n",
    "- output matrix $\\mathbf{Y}$\n",
    "\n",
    "$$\\mathbf{Y} = \\mathbf{X} \\star_{\\text{valid}} \\mathbf{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward propagation for convolutional layer\n",
    "\n",
    "- accumulated gradient from other layers $\\delta$\n",
    "\n",
    "- gradient of the loss function $\\mathcal{L}$ with respect to input matrix $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}}$\n",
    "\n",
    "- gradient of the loss function $\\mathcal{L}$ with respect to kernel $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{k}}$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{X}} = \\delta \\ast_{\\text{full}} \\mathbf{k} \\quad \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{k}} = \\mathbf{X} \\star_{\\text{valid}} \\delta $$\n",
    "\n",
    "- if stride greater than 1 is present, $\\delta$ needs to be dilated and padded to match shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilation\n",
    "\n",
    "- inserting zeroes between consecutive elements\n",
    "\n",
    "- used for pixel skipping, just like stride skips pixels\n",
    "\n",
    "- $\\text{stride} - 1$ zeroes are inserted between elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dilation](../img/conv_dilate.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def convolve2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for convolving 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of convolution.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    # Flip the kernel for convolution\n",
    "    kernel_flipped = np.rot90(kernel, 2) # or kernel_flipped = np.flipud(np.fliplr(kernel))\n",
    "\n",
    "    if mode == 'valid':\n",
    "    \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the convolution\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "    \n",
    "    elif mode == 'full':\n",
    "\n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "\n",
    "        # Pad input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km - 1, km - 1), (kn - 1, kn - 1)), mode='constant')\n",
    "\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel_flipped)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.   4.  -9. -12.]\n",
      " [  7.   6.  -5. -11.]\n",
      " [  9.   8.  -6.  -7.]\n",
      " [  7.   4.  -8.  -7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='valid')\n",
    "# It is noticable that the rotation of kernel from convolution does not yield the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full convolution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.  -1.   3.  -1.  -5.  -4.   5.   6.]\n",
      " [ -7.  -3.   6.   0.  -8. -10.   9.  13.]\n",
      " [-12.  -7.  11.   4.  -9. -12.  10.  15.]\n",
      " [-10.  -8.   7.   6.  -5. -11.   8.  13.]\n",
      " [-12.  -9.   9.   8.  -6.  -7.   9.   8.]\n",
      " [-10.  -6.   7.   4.  -8.  -7.  11.   9.]\n",
      " [ -9.  -4.   8.   3.  -7.  -4.   8.   5.]\n",
      " [ -3.  -1.   3.   0.  -3.  -2.   3.   3.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = convolve2d(x, kernel, mode='full')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross correlation implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_correlate2d(matrix: np.ndarray, kernel: np.ndarray, mode: Literal['valid', 'full'] = 'valid') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function for cross correlating 2D input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : np.ndarray\n",
    "        Input array.\n",
    "\n",
    "    kernel : np.ndarray\n",
    "        Array which slides over the input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : np.ndarray\n",
    "        Result of cross correlation.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the input matrix and kernel\n",
    "    m, n = matrix.shape\n",
    "    km, kn = kernel.shape\n",
    "\n",
    "    if mode == 'valid':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m - km + 1\n",
    "        output_dim_n = n - kn + 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    elif mode == 'full':\n",
    "        \n",
    "        # Calculate the dimensions of the output matrix\n",
    "        output_dim_m = m + km - 1\n",
    "        output_dim_n = n + kn - 1\n",
    "        output = np.zeros((output_dim_m, output_dim_n))\n",
    "        \n",
    "        # Pad the input matrix with zeros\n",
    "        padded_matrix = np.pad(matrix, ((km-1, km-1), (kn-1, kn-1)), mode='constant')\n",
    "\n",
    "        # Perform the cross-correlation\n",
    "        for i in range(output_dim_m):\n",
    "            for j in range(output_dim_n):\n",
    "                # Element-wise multiplication and summation\n",
    "                region = padded_matrix[i:i+km, j:j+kn]\n",
    "                output[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid cross correlation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-11.  -4.   9.  12.]\n",
      " [ -7.  -6.   5.  11.]\n",
      " [ -9.  -8.   6.   7.]\n",
      " [ -7.  -4.   8.   7.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[3, 1, 0, 2, 5, 6],\n",
    "              [4, 2, 1, 1, 4 ,7],\n",
    "              [5, 4 ,0, 0, 1, 2],\n",
    "              [1, 2, 2, 1, 3, 4],\n",
    "              [6, 3, 1, 0, 5, 2],\n",
    "              [3, 1, 0, 1, 3, 3]])\n",
    "\n",
    "kernel = np.array([[-1, 0, 1],\n",
    "                   [-1, 0, 1],\n",
    "                   [-1, 0, 1]])\n",
    "\n",
    "y = cross_correlate2d(x, kernel, mode='valid')\n",
    "# Using cross correlation which does not rotate the kernel yields the same result as the first animation\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilate(arr: np.ndarray, stride: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Expands boundaries of an array by adding rows and columns of zeros between array elements.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to dilate.\n",
    "\n",
    "    stride : int\n",
    "        Number of zeroes added between a pair of elements.\n",
    "        NOTE: stride - 1 zeros are added between elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dilated_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Create a new array with appropriate size for dilation\n",
    "    dilated_shape = (arr.shape[0] - 1) * stride + 1, (arr.shape[1] - 1) * stride + 1\n",
    "    dilated = np.zeros(dilated_shape)\n",
    "    \n",
    "    # Place the original array elements into the dilated array\n",
    "    for i in range(arr.shape[0]):\n",
    "        for j in range(arr.shape[1]):\n",
    "            dilated[i * stride, j * stride] = arr[i, j]\n",
    "    \n",
    "    return dilated\n",
    "\n",
    "def pad_to_shape(arr: np.ndarray, target_shape: tuple) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adds padding to array so it matches target shape.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Array to pad.\n",
    "\n",
    "    target_shape : tuple\n",
    "        Shape of the array after padding.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    padded_arr : np.ndarray\n",
    "    \"\"\"\n",
    "    # Calculate padding needed\n",
    "    pad_height = target_shape[0] - arr.shape[0]\n",
    "    pad_width = target_shape[1] - arr.shape[1]\n",
    "    \n",
    "    if pad_height < 0 or pad_width < 0:\n",
    "        raise ValueError(\"Target shape must be larger than the array shape.\")\n",
    "    \n",
    "    pad_top = pad_height // 2\n",
    "    pad_bottom = pad_height - pad_top\n",
    "    pad_left = pad_width // 2\n",
    "    pad_right = pad_width - pad_left\n",
    "    \n",
    "    # Apply padding\n",
    "    padded = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dilate and pad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dilated:\n",
      "[[1. 0. 2.]\n",
      " [0. 0. 0.]\n",
      " [3. 0. 4.]]\n",
      "Dilated and padded:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 2. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 3. 0. 4. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "dilated = dilate(x, 2)\n",
    "print(f'Dilated:\\n{dilated}')\n",
    "\n",
    "dilated_padded = pad_to_shape(dilated, (5, 5))\n",
    "print(f'Dilated and padded:\\n{dilated_padded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Layer\n",
    "\n",
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_channels: int, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Convolutional layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        output_channels : int\n",
    "            Number of channels of the output array.\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a single kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "        # Unpack input_shape tuple\n",
    "        input_channels, input_height, input_width = input_shape\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "        output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output and kernel shapes\n",
    "        self.output_shape = (output_channels, output_height, output_width)\n",
    "        self.kernels_shape = (output_channels, input_channels, kernel_size, kernel_size)\n",
    "\n",
    "        # Initialize layer parameters\n",
    "        self.kernels = np.random.randn(*self.kernels_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the convolutional layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, output_channels, height, width)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Add bias to output\n",
    "        self.output += self.biases\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "                    if self.padding:\n",
    "                        inputs = np.pad(self.inputs[i, k], pad_width=self.padding, mode='constant')\n",
    "                    else:\n",
    "                        inputs = self.inputs[i, k].copy()\n",
    "                    # Output is the cross correlation in valid mode between the input and kernel\n",
    "                    self.output[i, j] += signal.correlate2d(inputs, self.kernels[j, k], mode=\"valid\")[::self.stride, ::self.stride]\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the convolutional layer. Creates gradient attributes with respect to kernels, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dkernels = np.zeros(self.kernels.shape)\n",
    "        self.dbiases = np.zeros(self.biases.shape)\n",
    "        self.dinputs = np.zeros(self.inputs.shape)\n",
    "\n",
    "        # Number of samples, first dimension\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        # Loop through each sample, output channel and input channel\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Gradient with respect to biases is the sum of deltas\n",
    "            self.dbiases += delta[i]\n",
    "\n",
    "            for j in range(self.output_channels):\n",
    "                for k in range(self.input_channels):\n",
    "\n",
    "                    if self.padding:\n",
    "                        \n",
    "                        input_padded = np.pad(self.inputs[i, k], pad_width=self.padding)\n",
    "\n",
    "                        dkernels = self._calculate_kernel_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(input_padded, delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                        # Since padding was used gradient needs to be unpadded to match shape\n",
    "                        dinputs = dinputs[self.padding:-self.padding, self.padding:-self.padding]\n",
    "\n",
    "                    else:\n",
    "                        dkernels = self._calculate_kernel_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "                        dinputs = self._calculate_input_gradient(self.inputs[i, k], delta[i, j], self.kernels[j, k], stride=self.stride)\n",
    "\n",
    "                    self.dkernels[j, k] += dkernels\n",
    "                    self.dinputs[i, k] += dinputs\n",
    "\n",
    "    def _calculate_kernel_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Helper method for calculating kernel gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        kernel_grad : np.ndarray\n",
    "            Kernel gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            # If stride is present delta needs to be dilated\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_height, delta_dilated_width = delta_dilated.shape[-2:]\n",
    "            input_height, input_width = inputs.shape[-2:]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_height == input_height - kernel_shape + 1 and delta_dilated_width == input_width - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed correlation shape gradient can be computed\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated, \"valid\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed correlation shape padding is needed\n",
    "                new_delta_shape = (input_height - kernel_shape + 1, input_width - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dkernel = signal.correlate2d(inputs, delta_dilated_padded, \"valid\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to kernel is valid cross correlation between inputs and delta\n",
    "            dkernel = signal.correlate2d(inputs, delta, \"valid\")\n",
    "\n",
    "        return dkernel\n",
    "\n",
    "    def _calculate_input_gradient(self, inputs: np.ndarray, delta: np.ndarray, kernel: np.ndarray, stride: int = 1):\n",
    "        \"\"\"\n",
    "        Helper method for calculating input gradient.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Current sample the gradient is calculated for.\n",
    "\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        kernel : np.ndarray\n",
    "            Kernel used in convolutional layer.\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        input_grad : np.ndarray\n",
    "            Input gradient.\n",
    "        \"\"\"\n",
    "\n",
    "        if stride > 1:\n",
    "\n",
    "            delta_dilated = dilate(delta, stride)\n",
    "\n",
    "            delta_dilated_height, delta_dilated_width = delta_dilated.shape[-2:]\n",
    "            input_height, input_width = inputs.shape[-2:]\n",
    "            kernel_shape = kernel.shape[-1]\n",
    "\n",
    "            if delta_dilated_height == input_height - kernel_shape + 1 and delta_dilated_width == input_width - kernel_shape + 1:\n",
    "                # If dilated delta shape matches the needed coonvolution shape gradient can be computed\n",
    "                dinput = signal.convolve2d(delta_dilated, kernel, \"full\")\n",
    "            else:\n",
    "                # If dilated delta shape doesn't match the needed convolution shape padding is needed\n",
    "                new_delta_shape = (input_height - kernel_shape + 1, input_width - kernel_shape + 1)\n",
    "                delta_dilated_padded = pad_to_shape(delta_dilated, new_delta_shape)\n",
    "                dinput = signal.convolve2d(delta_dilated_padded, kernel, \"full\")\n",
    "\n",
    "        else:\n",
    "            # Gradient with respect to inputs is full convolution between delta and kernel\n",
    "            dinput = signal.convolve2d(delta, kernel, \"full\")\n",
    "\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, output_shape: tuple) -> None:\n",
    "        \"\"\"\n",
    "        Layer used to reshape (flatten) an array.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Input shape of a single sample. For images it's (channels, height, width).\n",
    "\n",
    "        output_shape : int\n",
    "            Output shape of a single sample.\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array to output shape. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : np.ndarray\n",
    "            Array to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = inputs.shape[0]\n",
    "        self.output = np.reshape(inputs, (batch_size, *self.output_shape))\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Reshapes input array to input shape. Creates gradient attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient to reshape.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store number of samples, first dimension\n",
    "        batch_size = delta.shape[0]\n",
    "        self.dinputs = np.reshape(delta, (batch_size, *self.input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maxpool layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_shape: tuple, kernel_size: int, stride: int = 1, padding: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Maxpooling layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape : tuple\n",
    "            Dimension of a single sample processed by the layer. For images it's (channels, width, height).\n",
    "\n",
    "        kernel_size : int\n",
    "            Dimension of a kernel, square array of shape (kernel_size, kernel_size).\n",
    "\n",
    "        stride : int, default=1\n",
    "            Step size at which the kernel moves across the input.\n",
    "\n",
    "        padding : int, default=0\n",
    "            Amount of padding added to input.\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the input_shape tuple\n",
    "        input_channels, input_height, input_width = input_shape\n",
    "\n",
    "        # Store input channels, kernel size and stride\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # Calculate output width and height\n",
    "        self.output_height = int((input_height - kernel_size + 2 * padding) / stride) + 1\n",
    "        self.output_width = int((input_width - kernel_size + 2 * padding) / stride) + 1\n",
    "\n",
    "        # Create output shape\n",
    "        self.output_shape = (self.input_channels, self.output_height, self.output_width)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the maxpool layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # List for storing indices of max elements (used in backward pass)\n",
    "        self.max_indices = []\n",
    "        \n",
    "        # Store inputs\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = inputs.shape[0]\n",
    "\n",
    "        # Output is 4D tensor of shape (n_samples, input_channels, width, height)\n",
    "        self.output = np.zeros((n_samples, *self.output_shape))\n",
    "\n",
    "        # Loop through every sample\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Add empty list to max indices for the current sample\n",
    "            self.max_indices.append([])\n",
    "\n",
    "            # Loop through every channel\n",
    "            for j in range(self.input_channels):\n",
    "\n",
    "                # Add empty list to max indices for the current channel of the current sample\n",
    "                self.max_indices[i].append([])\n",
    "\n",
    "                if self.padding:\n",
    "                    arr = np.pad(self.inputs[i, j], pad_width=self.padding, mode='constant')\n",
    "                else:\n",
    "                    arr = self.inputs[i, j].copy()\n",
    "\n",
    "                # Loop through each element of the output\n",
    "                for k in range(self.output_height):\n",
    "\n",
    "                    # Initalize axis 0 start and end indices \n",
    "                    axis_0_start = k * self.stride\n",
    "                    axis_0_end = axis_0_start + self.kernel_size\n",
    "\n",
    "                    for l in range(self.output_width):\n",
    "                        \n",
    "                        # Initalize axis 1 start and end indices\n",
    "                        axis_1_start = l*self.stride\n",
    "                        axis_1_end = axis_1_start + self.kernel_size\n",
    "                            \n",
    "                        # Use axis 0 and 1 indices to obtain max pooling region   \n",
    "                        region = arr[axis_0_start:axis_0_end, axis_1_start:axis_1_end]\n",
    "\n",
    "                        # Get the max element from the region, save it to output\n",
    "                        self.output[i, j, k, l] = np.max(region)\n",
    "                        \n",
    "                        # Get the index of the max element within the region (region is flattened array in this case)\n",
    "                        max_index = np.argmax(region)\n",
    "\n",
    "                        # Calculate the position of the max element within the sample\n",
    "                        max_element_position = (axis_0_start + (max_index // self.kernel_size), axis_1_start + (max_index % self.kernel_size))\n",
    "\n",
    "                        # Store the position of max element\n",
    "                        self.max_indices[i][j].append(max_element_position)\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the maxpool layer. Creates gradient attribute with respect to inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize inputs gradient\n",
    "        input_shape = self.inputs.shape\n",
    "        self.dinputs = np.zeros(input_shape)\n",
    "\n",
    "        # Number of samples, first dimenison\n",
    "        n_samples = self.inputs.shape[0]\n",
    "\n",
    "        if self.padding:\n",
    "            dinput_height, dinput_width = input_shape[-2:]\n",
    "            dinput_shape = (dinput_height + 2 * self.padding, dinput_width + 2 * self.padding)\n",
    "        else:\n",
    "            dinput_shape = input_shape[-2:]\n",
    "\n",
    "        # Loop through samples\n",
    "        for i in range(n_samples):\n",
    "            # Loop through channels\n",
    "            for j in range(self.input_channels):\n",
    "                \n",
    "                # Initialize gradient for current sample\n",
    "                dinput = np.zeros(dinput_shape)\n",
    "\n",
    "                # Loop through pairs of indices zipped with a delta value\n",
    "                for (k, l), d in zip(self.max_indices[i][j], delta[i, j].flatten()):\n",
    "                    dinput[k, l] = d\n",
    "\n",
    "                if self.padding:\n",
    "                    self.dinputs[i, j] = dinput[self.padding:-self.padding, self.padding:-self.padding]\n",
    "                else:\n",
    "                    self.dinputs[i, j] = dinput.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def load_mnist_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the header information\n",
    "        magic_number, num_images, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        # Read the image data\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num_images, rows, cols)\n",
    "        return images\n",
    "    \n",
    "def load_mnist_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        # Read the header information\n",
    "        magic_number, num_labels = struct.unpack(\">II\", f.read(8))\n",
    "        # Read the label data\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        return labels\n",
    "    \n",
    "def load_mnist(path_train_data, path_train_labels, path_test_data, path_test_labels):\n",
    "    X_train = load_mnist_images(path_train_data)\n",
    "    y_train = load_mnist_labels(path_train_labels)\n",
    "\n",
    "    X_test = load_mnist_images(path_test_data)\n",
    "    y_test = load_mnist_labels(path_test_labels)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28), y_train: (60000,)\n",
      "X_test: (10000, 28, 28), y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "path_train_data = \"../mnist/train-images.idx3-ubyte\"\n",
    "path_train_labels = \"../mnist/train-labels.idx1-ubyte\"\n",
    "\n",
    "path_test_data = \"../mnist/t10k-images.idx3-ubyte\"\n",
    "path_test_labels = \"../mnist/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist(path_train_data, path_train_labels, path_test_data, path_test_labels)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (200, 1, 28, 28), y_train: (200,)\n",
      "X_test: (200, 1, 28, 28), y_test: (200,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    zero_index = np.where(y == 0)[0][:limit]\n",
    "    one_index = np.where(y == 1)[0][:limit]\n",
    "    all_indices = np.hstack((zero_index, one_index))\n",
    "    all_indices = np.random.permutation(all_indices)\n",
    "    x, y = x[all_indices], y[all_indices]\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = preprocess_data(X_train, y_train, 100)\n",
    "X_test, y_test = preprocess_data(X_test, y_test, 100)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 0.68786 =====\n",
      "===== EPOCH : 5 ===== LOSS : 0.55265 =====\n",
      "===== EPOCH : 10 ===== LOSS : 0.27739 =====\n",
      "===== EPOCH : 15 ===== LOSS : 0.07313 =====\n",
      "===== EPOCH : 20 ===== LOSS : 0.02229 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.layers import DenseLayer, ConvolutionalLayer, MaxPoolLayer, ReshapeLayer\n",
    "from dlfs.activation import Sigmoid\n",
    "from dlfs.loss import BCE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "from dlfs import Model\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=2, padding=1, stride=2),\n",
    "          MaxPoolLayer(input_shape=(3, 15, 15), kernel_size=3, stride=2), \n",
    "          ReshapeLayer(input_shape=(3, 7, 7), output_shape=(3*7*7, )),\n",
    "          DenseLayer(3*7*7, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 1),\n",
    "          Sigmoid()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=BCE_Loss(), optimizer=Optimizer_SGD(learning_rate=8e-4, momentum=0.9, decay=1e-3))\n",
    "\n",
    "model.train(X_train, y_train.reshape(-1, 1), print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(f'Model accuracy: {np.mean(np.round(y_pred) == y_test.reshape(-1, 1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "for idx, x in enumerate(X_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.round(y_pred[0, 0])}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of kernels learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(8, 8))\n",
    "\n",
    "conv = model.layers[0]\n",
    "\n",
    "for i in range(conv.output_channels):\n",
    "    for j in range(conv.input_channels):\n",
    "\n",
    "        x = conv.kernels[i, j]\n",
    "        ax[i].imshow(x, cmap='gray')\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1000, 1, 28, 28), y_train: (1000, 10)\n",
      "X_test: (200, 1, 28, 28), y_test: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_whole_mnist(x):\n",
    "    x = x.reshape(len(x), 1, 28, 28)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    return x\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    categories = np.unique(y)\n",
    "    encoded_y = np.zeros((len(y), len(categories)))\n",
    "\n",
    "    for idx, label in enumerate(y):\n",
    "        to_encode_idx = np.argwhere(categories == label)\n",
    "        encoded_y[idx, to_encode_idx] = 1\n",
    "\n",
    "    return encoded_y\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_mnist(path_train_data, path_train_labels, path_test_data, path_test_labels)\n",
    "\n",
    "X_train = preprocess_whole_mnist(X_train[:1000])\n",
    "X_test = preprocess_whole_mnist(X_test[:200])\n",
    "\n",
    "y_train = one_hot_encode(y_train[:1000])\n",
    "y_test = one_hot_encode(y_test[:200])\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Loss, Activation\n",
    "\n",
    "class CCE_Loss(Loss):\n",
    "\n",
    "    def calculate(self, y_pred, y_true):\n",
    "        samples = range(len(y_pred))\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[samples, y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "\n",
    "        return (-np.sum(np.log(correct_confidences)))\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if(len(y_true.shape)) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples   \n",
    "\n",
    "class Softmax(Activation):\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp / np.sum(exp, axis=1, keepdims=True) \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    "\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2556.48965 =====\n",
      "===== EPOCH : 5 ===== LOSS : 2205.21975 =====\n",
      "===== EPOCH : 10 ===== LOSS : 2004.26818 =====\n",
      "===== EPOCH : 15 ===== LOSS : 1813.28476 =====\n",
      "===== EPOCH : 20 ===== LOSS : 1636.62789 =====\n"
     ]
    }
   ],
   "source": [
    "from dlfs.optimizers import Optimizer_Adam\n",
    "\n",
    "layers = [ConvolutionalLayer(input_shape=(1, 28, 28), output_channels=3, kernel_size=3, stride=2, padding=1),\n",
    "          MaxPoolLayer(input_shape=(3, 14, 14), kernel_size=3, stride=2, padding=2), \n",
    "          ReshapeLayer(input_shape=(3, 8, 8), output_shape=(3*8*8, )),\n",
    "          DenseLayer(3*8*8, 100),\n",
    "          Sigmoid(),\n",
    "          DenseLayer(100, 10),\n",
    "          Softmax()]\n",
    "\n",
    "model = Model(layers=layers, loss_function=CCE_Loss(), optimizer=Optimizer_Adam(learning_rate=5e-3, decay=1e-2))\n",
    "\n",
    "model.train(X_train, y_train, print_every=5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "i, j = 0, 0\n",
    "\n",
    "np.random.shuffle(X_test)\n",
    "\n",
    "for idx, x in enumerate(X_test[:6]):\n",
    "\n",
    "    img = x.reshape(28, 28)\n",
    "    x = x.reshape(1, *x.shape)\n",
    "    y_pred = model.predict(x)\n",
    "\n",
    "    ax[i, j].imshow(img, cmap='gray')\n",
    "    ax[i, j].set_xticks([])\n",
    "    ax[i, j].set_yticks([])\n",
    "    ax[i, j].set_title(f'Label: {np.argmax(y_pred.reshape(-1))}')\n",
    "\n",
    "    j += 1\n",
    "    if j % 3 == 0:\n",
    "        i += 1\n",
    "        j = 0\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
