{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "- used with sequential data (time series, sentences...)\n",
    "\n",
    "- parameters: input weights $\\mathbf{W}_i$, hidden weights $\\mathbf{W}_h$ and output weights $\\mathbf{W}_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{z}_t = \\mathbf{h}_{t-1} \\cdot \\mathbf{W}_h + \\mathbf{x}_t \\cdot \\mathbf{W}_i + \\mathbf{b}_i$$\n",
    "$$\\mathbf{h}_{t} = \\phi(\\mathbf{z}_t)$$\n",
    "$$\\mathbf{y}_t = \\mathbf{h}_{t} \\cdot \\mathbf{W}_o + \\mathbf{b}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{h}_{t-1}} = \\mathbf{W}_h \\quad \\quad \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{W}_h} = \\mathbf{h}_{t-1} \\quad \\quad \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{x}} = \\mathbf{W}_i \\quad \\quad \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{W}_i} = \\mathbf{x} \\quad \\quad \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{b}_i} = 1$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{z}_t} = \\phi'(\\mathbf{z}_t)$$\n",
    "\n",
    "$$\\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{h}_{t}} = \\mathbf{W}_o \\quad \\quad \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{W}_o} = \\mathbf{h}_{t} \\quad \\quad \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{b}_o} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{W}_o} &= \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\cdot \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{W}_o} \\\\\n",
    "&= \\delta_t \\cdot \\mathbf{h}_t\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_o} = \\sum_t \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{W}_o}\n",
    "\\end{aligned}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{b}_o} &= \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\cdot \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{b}_o} \\\\\n",
    "&= \\delta_t \\cdot 1 \\\\\n",
    "&= \\delta_t\n",
    "\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_o} = \\sum_t \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{b}_o}\n",
    "\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{W}_h} &= \\left( \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\cdot \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{h}_{t}} + \\mathbf{g}_{t+1} \\cdot \\frac{\\partial \\mathbf{z}_{t+1}}{\\partial \\mathbf{h}_{t}} \\right) \\cdot \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{z}_t} \\cdot \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{W}_h} \\\\\n",
    "\n",
    "&= \\left( \\delta_t \\cdot \\mathbf{W}_o + \\mathbf{g}_{t+1} \\cdot \\mathbf{W}_h \\right) \\cdot \\phi'(\\mathbf{z}_t)\\cdot \\mathbf{h}_{t-1} \\\\\n",
    "\n",
    "&= \\left[\\mathbf{d}_t \\odot \\phi'(\\mathbf{z}_t) \\right] \\cdot \\mathbf{h}_{t-1} \\\\\n",
    "\n",
    "&= \\mathbf{g}_t \\cdot \\mathbf{h}_{t-1}\n",
    "\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_h} = \\sum_t \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{W}_h}\n",
    "\\end{aligned}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{W}_i} &= \\left( \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\cdot \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{h}_{t}} + \\mathbf{g}_{t+1} \\cdot \\frac{\\partial \\mathbf{z}_{t+1}}{\\partial \\mathbf{h}_{t}} \\right) \\cdot \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{z}_t} \\cdot \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{W}_i} \\\\\n",
    "\n",
    "&= \\left( \\delta_t \\cdot \\mathbf{W}_o + \\mathbf{g}_{t+1} \\cdot \\mathbf{W}_h \\right) \\cdot \\phi'(\\mathbf{z}_t)\\cdot \\mathbf{x}_{t} \\\\\n",
    "\n",
    "&= \\left[\\mathbf{d}_t \\odot \\phi'(\\mathbf{z}_t) \\right] \\cdot \\mathbf{x}_{t} \\\\\n",
    "\n",
    "&= \\mathbf{g}_t \\cdot \\mathbf{x}_{t}\n",
    "\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{W}_i} = \\sum_t \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{W}_i}\n",
    "\\end{aligned}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\begin{aligned}\n",
    "\n",
    "\\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{b}_i} &= \\left( \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{y}_t} \\cdot \\frac{\\partial \\mathbf{y}_t}{\\partial \\mathbf{h}_{t}} + \\mathbf{g}_{t+1} \\cdot \\frac{\\partial \\mathbf{z}_{t+1}}{\\partial \\mathbf{h}_{t}} \\right) \\cdot \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{z}_t} \\cdot \\frac{\\partial \\mathbf{z}_t}{\\partial \\mathbf{b}_i} \\\\\n",
    "\n",
    "&= \\left( \\delta_t \\cdot \\mathbf{W}_o + \\mathbf{g}_{t+1} \\cdot \\mathbf{W}_h \\right) \\cdot \\phi'(\\mathbf{z}_t)\\cdot 1 \\\\\n",
    "\n",
    "&= \\mathbf{d}_t \\odot \\phi'(\\mathbf{z}_t) \\\\\n",
    "\n",
    "&= \\mathbf{g}_t\n",
    "\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{b}_i} = \\sum_t \\frac{\\partial \\mathcal{L}_t}{\\partial \\mathbf{b}_i}\n",
    "\\end{aligned}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{1}{2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs.base import Layer\n",
    "\n",
    "class RecurrentLayer(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs: int, n_hidden: int, n_outputs: int) -> None:\n",
    "        \"\"\"\n",
    "        Recurrent layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of input features.\n",
    "\n",
    "        n_hidden : int\n",
    "            Number of hidden features.\n",
    "\n",
    "        n_outputs : int\n",
    "            Number of output features.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        input_weights : numpy.ndarray\n",
    "            Matrix of input weight coefficients.\n",
    "\n",
    "        hidden_weights : numpy.ndarray\n",
    "            Matrix of hidden weight coefficients.\n",
    "\n",
    "        output_weights : numpy.ndarray\n",
    "            Matrix of output weight coefficients.\n",
    "\n",
    "        input_bias : numpy.ndaray\n",
    "            Vector of input bias coefficients.\n",
    "\n",
    "        output_bias : numpy.ndaray\n",
    "            Vector of output bias coefficients.\n",
    "        \"\"\"\n",
    "        k = 1 / np.sqrt(n_hidden)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_weights = np.random.uniform(-k, k, (n_inputs, n_hidden))\n",
    "        self.hidden_weights = np.random.uniform(-k, k, (n_hidden, n_hidden))\n",
    "        self.output_weights = np.random.uniform(-k, k, (n_hidden, n_outputs))\n",
    "        self.input_bias = np.random.uniform(-k, k, (n_hidden))\n",
    "        self.output_bias = np.random.uniform(-k, k, (n_outputs))\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the recurrent layer. Creates hidden states and output attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Store number of samples\n",
    "        self.n_samples = inputs.shape[0]\n",
    "\n",
    "        self.sequence_length = inputs.shape[1]\n",
    "\n",
    "        # Initialize output\n",
    "        self.output = np.zeros((self.n_samples, self.sequence_length, self.output_weights.shape[1]))\n",
    "\n",
    "        # Initialize hidden states\n",
    "        self.hidden_states = np.zeros((self.n_samples, self.sequence_length, self.n_hidden))\n",
    "\n",
    "        for i, sequence in enumerate(inputs):\n",
    "\n",
    "            for j, x in enumerate(sequence):\n",
    "\n",
    "                # Reshape to match dimensions\n",
    "                x = x.reshape(1, -1)\n",
    "\n",
    "                input_x = np.dot(x, self.input_weights)\n",
    "\n",
    "                hidden_x = input_x + np.dot(self.hidden_states[i, max(j-1, 0)], self.hidden_weights) + self.input_bias\n",
    "\n",
    "                # Activation function\n",
    "                hidden_x = np.tanh(hidden_x)\n",
    "\n",
    "                # Store current hidden state\n",
    "                self.hidden_states[i, j] = hidden_x.copy()\n",
    "\n",
    "                output_x = np.dot(hidden_x, self.output_weights) + self.output_bias\n",
    "\n",
    "                # Store current output\n",
    "                self.output[i, j] = output_x.copy()\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the recurrent layer. \n",
    "        Creates gradient attributes with respect to input weights, hidden weights, output weights, input bias, output bias and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dinput_weights = np.zeros_like(self.input_weights)\n",
    "        self.dhidden_weights = np.zeros_like(self.hidden_weights)\n",
    "        self.dinput_bias = np.zeros_like(self.input_bias)\n",
    "        self.doutput_weights = np.zeros_like(self.output_weights)\n",
    "        self.doutput_bias = np.zeros_like(self.output_bias)\n",
    "        self.dinputs = np.zeros_like(self.inputs, dtype=np.float64)\n",
    "\n",
    "        for i in range(self.n_samples - 1, -1, -1):\n",
    "\n",
    "            # Initialize next hidden gradient\n",
    "            next_hidden_gradient = None\n",
    "\n",
    "            for j in range(self.sequence_length - 1, -1, -1):\n",
    "\n",
    "                loss_gradient = delta[i, j].reshape(1, -1)\n",
    "                hidden_state = self.hidden_states[i, j].reshape(-1, 1)\n",
    "\n",
    "                self.doutput_weights += np.dot(hidden_state, loss_gradient)\n",
    "                self.doutput_bias += loss_gradient.reshape(-1)\n",
    "\n",
    "                hidden_gradient = np.dot(loss_gradient, self.output_weights.T)\n",
    "                if next_hidden_gradient is not None:\n",
    "                    hidden_gradient += np.dot(next_hidden_gradient, self.hidden_weights.T)\n",
    "\n",
    "                dtanh = 1 - self.hidden_states[i, j]**2\n",
    "                hidden_gradient *= dtanh\n",
    "\n",
    "                next_hidden_gradient = hidden_gradient.copy()\n",
    "\n",
    "                if j > 0:\n",
    "                    self.dhidden_weights += np.dot(self.hidden_states[i, j-1].reshape(-1, 1), hidden_gradient)\n",
    "\n",
    "                self.dinput_weights += np.dot(self.inputs[i, j].reshape(-1, 1), hidden_gradient)\n",
    "                self.dinput_bias += hidden_gradient.reshape(-1)\n",
    "                \n",
    "                self.dinputs[i, j] += np.dot(self.input_weights, hidden_gradient.T).reshape(-1)\n",
    "\n",
    "class RecurrentLayerHidden(Layer):\n",
    "\n",
    "    def __init__(self, n_inputs: int, n_hidden: int) -> None:\n",
    "        \"\"\"\n",
    "        Recurrent layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of input features.\n",
    "\n",
    "        n_hidden : int\n",
    "            Number of hidden features.\n",
    "\n",
    "        n_outputs : int\n",
    "            Number of output features.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        input_weights : numpy.ndarray\n",
    "            Matrix of input weight coefficients.\n",
    "\n",
    "        hidden_weights : numpy.ndarray\n",
    "            Matrix of hidden weight coefficients.\n",
    "\n",
    "        output_weights : numpy.ndarray\n",
    "            Matrix of output weight coefficients.\n",
    "\n",
    "        input_bias : numpy.ndaray\n",
    "            Vector of input bias coefficients.\n",
    "\n",
    "        output_bias : numpy.ndaray\n",
    "            Vector of output bias coefficients.\n",
    "        \"\"\"\n",
    "        k = 1 / np.sqrt(n_hidden)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.input_weights = np.random.uniform(-k, k, (n_inputs, n_hidden))\n",
    "        self.hidden_weights = np.random.uniform(-k, k, (n_hidden, n_hidden))\n",
    "        self.input_bias = np.random.uniform(-k, k, (n_hidden))\n",
    "        \n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the recurrent layer. Creates hidden states and output attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store inputs for later use\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Store number of samples\n",
    "        self.n_samples = inputs.shape[0]\n",
    "\n",
    "        self.sequence_length = inputs.shape[1]\n",
    "\n",
    "        # Initialize output\n",
    "        self.output = np.zeros((self.n_samples, self.n_hidden))\n",
    "\n",
    "        # Initialize hidden states\n",
    "        self.hidden_states = np.zeros((self.n_samples, self.sequence_length, self.n_hidden))\n",
    "\n",
    "        for i, sequence in enumerate(inputs):\n",
    "\n",
    "            for j, x in enumerate(sequence):\n",
    "\n",
    "                # Reshape to match dimensions\n",
    "                x = x.reshape(1, -1)\n",
    "\n",
    "                input_x = np.dot(x, self.input_weights)\n",
    "\n",
    "                hidden_x = input_x + np.dot(self.hidden_states[i, max(j-1, 0)], self.hidden_weights) + self.input_bias\n",
    "\n",
    "                # Activation function\n",
    "                hidden_x = np.tanh(hidden_x)\n",
    "\n",
    "                # Store current hidden state\n",
    "                self.hidden_states[i, j] = hidden_x.copy()\n",
    "\n",
    "            # Store current output\n",
    "            self.output[i] = self.hidden_states[i, -1].copy()\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the recurrent layer. \n",
    "        Creates gradient attributes with respect to input weights, hidden weights, output weights, input bias, output bias and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Initialize gradient attributes\n",
    "        self.dinput_weights = np.zeros_like(self.input_weights)\n",
    "        self.dhidden_weights = np.zeros_like(self.hidden_weights)\n",
    "        self.dinput_bias = np.zeros_like(self.input_bias)\n",
    "        self.dinputs = np.zeros_like(self.inputs, dtype=np.float64)\n",
    "\n",
    "        for i in range(self.n_samples - 1, -1, -1):\n",
    "\n",
    "            # Initialize next hidden gradient\n",
    "            next_hidden_gradient = None\n",
    "\n",
    "            for j in range(self.sequence_length - 1, -1, -1):\n",
    "\n",
    "                loss_gradient = delta[i].reshape(1, -1)\n",
    "\n",
    "                hidden_gradient = loss_gradient.copy()\n",
    "                if next_hidden_gradient is not None:\n",
    "                    hidden_gradient += np.dot(next_hidden_gradient, self.hidden_weights)\n",
    "\n",
    "                dtanh = 1 - self.hidden_states[i, j]**2\n",
    "                hidden_gradient *= dtanh\n",
    "\n",
    "                next_hidden_gradient = hidden_gradient.copy()\n",
    "\n",
    "                if j > 0:\n",
    "                    self.dhidden_weights += np.dot(self.hidden_states[i, j-1].reshape(-1, 1), hidden_gradient)\n",
    "\n",
    "                self.dinput_weights += np.dot(self.inputs[i, j].reshape(-1, 1), hidden_gradient)\n",
    "                self.dinput_bias += hidden_gradient.reshape(-1)\n",
    "                \n",
    "                self.dinputs[i, j] += np.dot(self.input_weights, hidden_gradient.T).reshape(-1)\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, n_inputs: int, n_hidden: int, n_layers: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        Recurrent neural network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of input features.\n",
    "\n",
    "        n_hidden : int\n",
    "            Number of hidden features.\n",
    "        \"\"\"\n",
    "        self.recurrent_layers = [RecurrentLayerHidden(n_inputs, n_hidden)]\n",
    "        if n_layers > 1:\n",
    "            for _ in range(n_layers - 1):\n",
    "                self.recurrent_layers.append(RecurrentLayerHidden(n_hidden, n_hidden))\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "\n",
    "        self.recurrent_layers[0].forward(inputs)\n",
    "\n",
    "        for idx, layer in enumerate(self.recurrent_layers[1:], start=1):\n",
    "            layer.forward(self.recurrent_layers[idx - 1].hidden_states)\n",
    "\n",
    "        self.output = self.recurrent_layers[-1].output.copy()\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "\n",
    "        self.recurrent_layers[-1].backward(delta)\n",
    "\n",
    "        for idx, layer in reversed(list(enumerate(self.recurrent_layers[:-1]))):\n",
    "            layer.backward(self.recurrent_layers[idx + 1].dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_sequence(X, y, sequence_length):\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        X_new.append(X[i:i+sequence_length, :])\n",
    "        y_new.append(y[i+sequence_length])\n",
    "\n",
    "    X_new, y_new = np.array(X_new), np.array(y_new)\n",
    "\n",
    "    return X_new, y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sequence = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "sequence = scaler.fit_transform(sequence.reshape(-1, 1))\n",
    "result = np.array([60, 52, 52, 53, 52, 50, 52, 56, 54, 57]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: (10, 1), results: (10, 1)\n",
      "Sequence new: (7, 3, 1), results new: (7, 1)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 3\n",
    "sequence_new, result_new = convert_data_to_sequence(sequence, result, seq_len)\n",
    "\n",
    "print(f'Sequence: {sequence.shape}, results: {result.shape}')\n",
    "print(f'Sequence new: {sequence_new.shape}, results new: {result_new.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlfs import Model\n",
    "from dlfs.layers import RecurrentLayer\n",
    "from dlfs.loss import MSE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "layers = [RecurrentLayer(1, 5, 3), \n",
    "          RecurrentLayer(3, 7, 1)]\n",
    "lr = 5e-3\n",
    "\n",
    "model = Model(layers=layers, loss_function=MSE_Loss(), optimizer=Optimizer_SGD(learning_rate=lr))\n",
    "\n",
    "#model.train(sequence_new, result_new, epochs=500, batch_size=None, print_every=100)\n",
    "#print(f'{model.predict(sequence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 1425.2647383778499 =====\n",
      "===== EPOCH : 100 ===== LOSS : 1.3774086216882584 =====\n",
      "===== EPOCH : 200 ===== LOSS : 1.3768691843246585 =====\n",
      "===== EPOCH : 300 ===== LOSS : 1.3763303774884217 =====\n",
      "===== EPOCH : 400 ===== LOSS : 1.3757920345979413 =====\n",
      "===== EPOCH : 500 ===== LOSS : 1.3752541552485338 =====\n",
      "[[45.39529815]\n",
      " [46.22327222]\n",
      " [47.04763335]\n",
      " [47.86604969]\n",
      " [48.67621924]\n",
      " [49.47590003]\n",
      " [50.26293922]\n",
      " [51.0353003 ]\n",
      " [51.79108747]\n",
      " [52.52856648]]\n"
     ]
    }
   ],
   "source": [
    "from dlfs import Model\n",
    "from dlfs.layers import RecurrentLayer, DenseLayer\n",
    "from dlfs.loss import MSE_Loss\n",
    "from dlfs.optimizers import Optimizer_SGD\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "layers = [RecurrentLayerHidden(1, 50), \n",
    "          DenseLayer(50, 1)]\n",
    "lr = 5e-2\n",
    "\n",
    "#result_new = np.array([53, 52, 50, 52, 56, 54, 57, 56]).reshape(-1, 1)\n",
    "\n",
    "model = Model(layers=layers, loss_function=MSE_Loss(), optimizer=Optimizer_SGD(learning_rate=lr))\n",
    "\n",
    "model.train(sequence_new, result_new, epochs=500, batch_size=None, print_every=100)\n",
    "print(f'{model.predict(sequence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tmax</th>\n",
       "      <th>tmin</th>\n",
       "      <th>rain</th>\n",
       "      <th>tmax_tomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-01</td>\n",
       "      <td>60.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-02</td>\n",
       "      <td>52.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-03</td>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-04</td>\n",
       "      <td>53.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-05</td>\n",
       "      <td>52.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1970-01-06</td>\n",
       "      <td>50.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1970-01-07</td>\n",
       "      <td>52.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1970-01-08</td>\n",
       "      <td>56.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1970-01-09</td>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1970-01-10</td>\n",
       "      <td>57.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  tmax  tmin  rain  tmax_tomorrow\n",
       "0  1970-01-01  60.0  35.0  0.00           52.0\n",
       "1  1970-01-02  52.0  39.0  0.00           52.0\n",
       "2  1970-01-03  52.0  35.0  0.00           53.0\n",
       "3  1970-01-04  53.0  36.0  0.00           52.0\n",
       "4  1970-01-05  52.0  35.0  0.00           50.0\n",
       "5  1970-01-06  50.0  38.0  0.00           52.0\n",
       "6  1970-01-07  52.0  43.0  0.00           56.0\n",
       "7  1970-01-08  56.0  49.0  0.24           54.0\n",
       "8  1970-01-09  54.0  50.0  0.40           57.0\n",
       "9  1970-01-10  57.0  50.0  0.00           57.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('clean_weather.csv', names=['date', 'tmax', 'tmin', 'rain', 'tmax_tomorrow'], header=0)\n",
    "data.head(10)\n",
    "#data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (13509, 3)\n",
      "y: (13509,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FEATURES = ['tmax', 'tmin', 'rain']\n",
    "TARGET = 'tmax_tomorrow'\n",
    "\n",
    "X = data[FEATURES].to_numpy()\n",
    "y = data[TARGET].to_numpy()\n",
    "\n",
    "print(f'X: {X.shape}')\n",
    "print(f'y: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (270, 3), y_train: (270,)\n",
      "X_test: (13239, 3), y_test: (13239,)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.02\n",
    "\n",
    "X_train = X[:int(threshold*len(X)),:].copy()\n",
    "y_train = y[:int(threshold*len(X))].copy()\n",
    "\n",
    "X_test = X[int(threshold*len(X)):,:].copy()\n",
    "y_test = y[int(threshold*len(X)):].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f'X_train: {X_train.shape}, y_train: {y_train.shape}')\n",
    "print(f'X_test: {X_test.shape}, y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (263, 7, 3), (263, 1)\n",
      "shapes: (13232, 7, 3), (13232, 1)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 7\n",
    "X_train_new, y_train_new = convert_data_to_sequence(X_train, y_train.reshape(-1, 1), seq_len)\n",
    "X_test_new, y_test_new = convert_data_to_sequence(X_test, y_test.reshape(-1, 1), seq_len)\n",
    "print(f'shapes: {X_train_new.shape}, {y_train_new.shape}')\n",
    "print(f'shapes: {X_test_new.shape}, {y_test_new.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 3664.86700090967 =====\n",
      "===== EPOCH : 10 ===== LOSS : 1755.8492319681461 =====\n",
      "===== EPOCH : 20 ===== LOSS : 1042.3108602829775 =====\n",
      "===== EPOCH : 30 ===== LOSS : 746.4693865325233 =====\n",
      "===== EPOCH : 40 ===== LOSS : 612.2120418247232 =====\n",
      "===== EPOCH : 50 ===== LOSS : 546.2726310394227 =====\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "layers = [RecurrentLayerHidden(3, 4),\n",
    "          DenseLayer(4, 1)]\n",
    "lr = 1e-4\n",
    "\n",
    "model = Model(layers=layers, loss_function=MSE_Loss(), optimizer=Optimizer_SGD(learning_rate=lr, momentum=0.5))\n",
    "model.train(X_train_new, y_train_new, epochs=50, batch_size=1, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_new, y_test_new = convert_data_to_sequence(X_test, y_test, sequence_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55.12179357]\n",
      " [54.42053747]\n",
      " [57.55431227]\n",
      " [60.38021396]\n",
      " [56.42478534]]\n",
      "[69. 59. 58. 60. 52.]\n",
      "Loss: 21.095672518281834\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "start = randint(0, len(X_test_new))\n",
    "y_pred = model.predict(X_test_new[start:start+5].reshape(5, *X_test_new[0].shape))\n",
    "print(y_pred)\n",
    "print(y_test[start:start+5])\n",
    "print(f'Loss: {model.loss_function.calculate(y_pred, y_test[start:start+5])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (263, 7, 3)\n",
      "y_train: (263, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train: {X_train_new.shape}')\n",
    "print(f'y_train: {y_train_new.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== EPOCH : 0 ===== LOSS : 2196.569995184542 =====\n",
      "===== EPOCH : 10 ===== LOSS : 28.844170899675913 =====\n",
      "===== EPOCH : 20 ===== LOSS : 202.39847466369582 =====\n",
      "===== EPOCH : 30 ===== LOSS : 40.41449498259472 =====\n",
      "===== EPOCH : 40 ===== LOSS : 26.10478566903436 =====\n",
      "===== EPOCH : 50 ===== LOSS : 24.833312614288186 =====\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "from dlfs.layers import RNN\n",
    "\n",
    "layers = [RNN(3, 1), DenseLayer(1, 1)]\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "model = Model(layers=layers, loss_function=MSE_Loss(), optimizer=Optimizer_SGD(learning_rate=lr, momentum=0.))\n",
    "model.train(X_train_new, y_train_new, epochs=50, batch_size=None, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263, 15)\n"
     ]
    }
   ],
   "source": [
    "print(layers[0].recurrent_layers[-1].output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2131.7148288973385"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_function.calculate(layers[0].recurrent_layers[-1].output, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(3, 4, n_layers=5)\n",
    "model.forward(X_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = MSE_Loss()\n",
    "y_pred = model.output.copy()\n",
    "loss_fn.backward(y_pred, y_train_new)\n",
    "#print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward(loss_fn.dinputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
