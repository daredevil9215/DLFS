{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "\n",
    "- \"vanilla\" feed-forward neural network\n",
    "\n",
    "- consists of an input layer, multiple hidden layers and an output layer (deep neural network => deep learning)\n",
    "\n",
    "- trained using the backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLP](img/mlp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "\n",
    "- passing data through the network until output $\\hat{\\mathbf{y}}$ is calculated\n",
    "\n",
    "- matrix multiplication of an input matrix $\\mathbf{X}$ and weights of every hidden layer $\\mathbf{W}_n$ plus a bias vector $\\mathbf{b}_n$ passed to an activation function $f_n$ until the output layer is reached\n",
    "\n",
    "- it is quite common for the output layer to have a different activation function as compared to the hidden layers\n",
    "\n",
    "- forward pass for the image above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{Z}_1 &= \\mathbf{X} \\cdot \\mathbf{W}_1 + \\mathbf{b}_1 \\\\\n",
    "\\mathbf{U}_1 &= f_1(\\mathbf{Z}_1) \\\\\n",
    "& \\text{Hidden layer 1}\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z}_2 &= \\mathbf{U}_1 \\cdot \\mathbf{W}_2 + \\mathbf{b}_2 \\\\\n",
    "\\mathbf{U}_2 &= f_2(\\mathbf{Z}_2) \\\\\n",
    "& \\text{Hidden layer 2}\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z}_3 &= \\mathbf{U}_2 \\cdot \\mathbf{W}_3 + \\mathbf{b}_3 \\\\\n",
    "\\mathbf{U}_3 &= f_3(\\mathbf{Z}_3) \\\\\n",
    "& \\text{Hidden layer 3}\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z}_4 &= \\mathbf{U}_3 \\cdot \\mathbf{W}_4 + \\mathbf{b}_4 \\\\\n",
    "\\mathbf{U}_4 &= f_4(\\mathbf{Z}_4) \\\\\n",
    "& \\text{Hidden layer 4}\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Z}_5 &= \\mathbf{U}_4 \\cdot \\mathbf{W}_5 + \\mathbf{b}_5 \\\\\n",
    "\\hat{\\mathbf{y}} &= f_5(\\mathbf{Z}_5) \\\\\n",
    "& \\text{Output layer} \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{Z}_1 &= \\mathbf{X} \\cdot \\mathbf{W}_1 + \\mathbf{b}_1 \\\\\n",
    "\\mathbf{U}_1 &= f_1(\\mathbf{Z}_1) \\\\\n",
    "&\\text{Hidden layer 1}\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{Z}_2 &= \\mathbf{U}_1 \\cdot \\mathbf{W}_2 + \\mathbf{b}_2 \\\\\n",
    "\\mathbf{U}_2 &= f_2(\\mathbf{Z}_2) \\\\\n",
    "&\\text{Hidden layer 2}\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{Z}_3 &= \\mathbf{U}_2 \\cdot \\mathbf{W}_3 + \\mathbf{b}_3 \\\\\n",
    "\\mathbf{U}_3 &= f_3(\\mathbf{Z}_3) \\\\\n",
    "&\\text{Hidden layer 3}\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{Z}_4 &= \\mathbf{U}_3 \\cdot \\mathbf{W}_4 + \\mathbf{b}_4 \\\\\n",
    "\\mathbf{U}_4 &= f_4(\\mathbf{Z}_4) \\\\\n",
    "&\\text{Hidden layer 4}\n",
    "\\end{aligned} \\quad \\quad\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{Z}_5 &= \\mathbf{U}_4 \\cdot \\mathbf{W}_5 + \\mathbf{b}_5 \\\\\n",
    "\\hat{\\mathbf{y}} &= f_5(\\mathbf{Z}_5) \\\\\n",
    "&\\text{Output layer}\n",
    "\\end{aligned}\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation (backward pass)\n",
    "\n",
    "- distributing gradients backwards through the network using the chain rule\n",
    "\n",
    "- gradient calculation starts at the output layer using the loss function $L(\\hat{y}, y)$\n",
    "\n",
    "- calculated gradients of network parameters (weights $\\mathbf{W}$ and biases $\\mathbf{b}$) are used to minimize the loss function using gradient descent\n",
    "\n",
    "- backward pass for the image above ($\\cdot$ denotes dot product, $\\odot$ denotes element-wise multiplication (Hadamard product)):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}_5} &= \\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} \\cdot \\frac{\\partial \\hat{\\mathbf{y}}}{\\partial \\mathbf{Z}_5} \\\\\n",
    "&= \\frac{\\partial L}{\\partial \\hat{\\mathbf{y}}} \\odot f_5'(\\mathbf{Z}_5) \\\\\n",
    "&= \\delta_{5}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_5} &=  \\frac{\\partial L}{\\partial \\mathbf{Z}_5} \\cdot \\frac{\\partial \\mathbf{Z}_5}{\\partial \\mathbf{W}_5} \\\\\n",
    "&= \\mathbf{U}_{4}^{\\intercal} \\cdot \\frac{\\partial L}{\\partial \\mathbf{Z}_5} \\\\\n",
    "&= \\mathbf{U}_{4}^{\\intercal} \\cdot \\delta_{5}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}_5} &=  \\frac{\\partial L}{\\partial \\mathbf{Z}_5} \\cdot \\frac{\\partial \\mathbf{Z}_5}{\\partial \\mathbf{b}_5} \\\\\n",
    "&= \\mathbf{U}_{4}^{\\intercal} \\cdot 1 \\\\\n",
    "&= \\sum \\frac{\\partial L}{\\partial \\mathbf{Z}_5}\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{U}_4} &= \\frac{\\partial L}{\\partial \\mathbf{Z}_5} \\cdot \\frac{\\partial \\mathbf{Z}_5}{\\partial \\mathbf{U}_4} \\\\\n",
    "&= \\frac{\\partial L}{\\partial \\mathbf{Z}_5} \\cdot \\mathbf{W}_{5}^{\\intercal} \\\\\n",
    "&= \\delta_{5} \\cdot \\mathbf{W}_{5}^{\\intercal}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}_4} &= \\frac{\\partial L}{\\partial \\mathbf{U}_4} \\cdot \\frac{\\partial \\mathbf{U}_4}{\\partial \\mathbf{Z}_4} \\\\\n",
    "&= \\left( \\delta_{5} \\cdot \\mathbf{W}_{5}^{\\intercal} \\right) \\odot f_4'(Z_4)\\\\\n",
    "&= \\delta_{4}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_4} &=  \\frac{\\partial L}{\\partial \\mathbf{Z}_4} \\cdot \\frac{\\partial \\mathbf{Z}_4}{\\partial \\mathbf{W}_4} \\\\\n",
    "&= \\mathbf{U}_{3}^{\\intercal} \\cdot \\frac{\\partial L}{\\partial \\mathbf{Z}_4} \\\\\n",
    "&= \\mathbf{U}_{3}^{\\intercal} \\cdot \\delta_{4}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}_4} &=  \\frac{\\partial L}{\\partial \\mathbf{Z}_4} \\cdot \\frac{\\partial \\mathbf{Z}_4}{\\partial \\mathbf{b}_4} \\\\\n",
    "&= \\delta_{4} \\cdot 1 \\\\\n",
    "&= \\sum \\delta_{4}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\vdots \\\\\n",
    "\\vdots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{U}_1} &= \\frac{\\partial L}{\\partial \\mathbf{Z}_2} \\cdot \\frac{\\partial \\mathbf{Z}_2}{\\partial \\mathbf{U}_1} \\\\\n",
    "&= \\frac{\\partial L}{\\partial \\mathbf{Z}_2} \\cdot \\mathbf{W}_{2}^{\\intercal} \\\\\n",
    "&= \\delta_{2} \\cdot \\mathbf{W}_{2}^{\\intercal}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{Z}_1} &= \\frac{\\partial L}{\\partial \\mathbf{U}_1} \\cdot \\frac{\\partial \\mathbf{U}_1}{\\partial \\mathbf{Z}_1} \\\\\n",
    "&= \\left( \\delta_{2} \\cdot \\mathbf{W}_{2}^{\\intercal} \\right) \\odot f_1'(Z_1)\\\\\n",
    "&= \\delta_{1}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}_1} &=  \\frac{\\partial L}{\\partial \\mathbf{Z}_1} \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{W}_1} \\\\\n",
    "&= \\mathbf{X}^{\\intercal} \\cdot \\frac{\\partial L}{\\partial \\mathbf{Z}_1} \\\\\n",
    "&= \\mathbf{X}^{\\intercal} \\cdot \\delta_{1}\n",
    "\n",
    "\\end{align*} \\quad \\quad\n",
    "\n",
    "\\begin{align*}\n",
    "\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}_1} &=  \\frac{\\partial L}{\\partial \\mathbf{Z}_1} \\cdot \\frac{\\partial \\mathbf{Z}_1}{\\partial \\mathbf{b}_1} \\\\\n",
    "&= \\delta_{1} \\cdot 1 \\\\\n",
    "&= \\sum \\delta_{1}\n",
    "\n",
    "\\end{align*}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, n_inputs: int, n_neurons: int) -> None:\n",
    "        \"\"\"\n",
    "        Layer of neurons consisting of a weight matrix and bias vector.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_inputs : int\n",
    "            Number of inputs that connect to the layer.\n",
    "\n",
    "        n_neurons : int\n",
    "            Number of neurons the layer consists of.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        weights : numpy.ndarray\n",
    "            Matrix of weight coefficients.\n",
    "\n",
    "        biases : numpy.ndaray\n",
    "            Vector of bias coefficients.\n",
    "        \"\"\"\n",
    "\n",
    "        # Weights are randomly initialized, small random numbers seem to work well\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Bias vector is initialized to a zero vector\n",
    "        self.biases = np.zeros(n_neurons)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using the layer. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store inputs for later use (backpropagation)\n",
    "        self.inputs = inputs\n",
    "        # Output is the dot product of the input matrix and weights plus biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, delta: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Backward pass using the layer. Creates gradient attributes for weights, biases and inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, delta)\n",
    "        self.dbiases = np.sum(delta, axis=0)\n",
    "        self.dinputs = np.dot(delta, self.weights.T)\n",
    "\n",
    "class ReLU:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Rectified Linear Unit activation function.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs) -> None:\n",
    "        \"\"\"\n",
    "        Forward pass using ReLU. Creates output attribute.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : numpy.ndarray\n",
    "            Input matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Store inputs for later use (backpropagation)\n",
    "        self.inputs = inputs\n",
    "        # Output is max value between 0 and inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        \"\"\"\n",
    "        Backward pass using ReLU. Creates gradient attribute for inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        delta : np.ndarray\n",
    "            Accumulated gradient obtained by backpropagation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.dinputs = delta.copy()\n",
    "        self.dinputs[self.inputs < 0] = 0\n",
    "\n",
    "a = ReLU()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
